---
title: |
  | Matching for Adjustment and Causal Inference
  | Class 3: Propensity Scores, Calipers, Exact Matching, Combining Distance Matrices
date: '`r format(Sys.Date(), "%B %d, %Y")`'
author: Jake Bowers
bibliography:
 - ../BIB/references.bib
fontsize: 10pt
geometry: margin=1in
graphics: yes
biblio-style: authoryear-comp
biblatexoptions:
  - natbib=true
output:
  beamer_presentation:
    slide_level: 2
    keep_tex: true
    latex_engine: lualatex
    citation_package: biblatex
    incremental: true
    template: icpsr.beamer
    includes:
        in_header:
           - defs-all.sty
    md_extensions: +raw_attribute-tex_math_single_backslash+autolink_bare_uris+ascii_identifiers+tex_math_dollars
header-includes:
  - \setbeameroption{hide notes}
---

<!-- To show notes  -->
<!-- https://stackoverflow.com/questions/44906264/add-speaker-notes-to-beamer-presentations-using-rmarkdown -->

```{r echo=FALSE, include=FALSE, cache=FALSE}
library(here)
## You may need to change this next line to the following if you are using the EIM repo
## source(here::here("rmd_setup.R"))
source(here::here("Exercises/rmd_setup.R"))
library(tidyverse)
library(optmatch)
library(RItools)
library(estimatr)
```

# Overview and Review

## Last Time

1. Yet more evidence that adjustment for background covariates using the linear
   model ("controlling for") is difficult: difficult to explain, difficult to
   justify and assess, etc.. Too many specifications to choose from, too
   difficult to assess the influence of functional form assumptions (let alone
   extrapolation and interpolation) with many covariates. Although we will use
   the linear model for estimation  we will move away from it for adjustment.
2. Stratification is an old and simple idea: hold constant by holding constant
   directly --- breaking continuous variables into pieces, or just estimating
   effects within groups. This is easy to explain. The adjustment is
   transparent.
3. Block-randomized experiments are well known and methods for estimating
   overall ATE from block-randomized studies are also well established: so
   stratification based approaches need not leave us with many imprecise
   treatment effects, for example. So, we can use the general techniques of
   combining block-specific or stratum-specific effects by weighting from that
   literature. This leaves us with two kinds of weights (a) block-size weights
   and (b) precision weights (which add the ratio of treated to control to its
   measure of information contributed to the overall estimate from a given
   block).
4. We can assess the success of a stratification by comparing it directly to a
   randomized experiment --- leading to a hypothesis test or a balance test
   (based on randomization as the standard of comparison).
5. We can assess the success of a stratification just by inspecting the blocks.
6. Optimal full matching (optimal following @rosenbaum2010, Chap 8 discussion
   and cites therein) creates stratifications that minimize differences between
   treated and control units --- this side-steps questions about cut-points or
   about numbers of groups. The number of sets is optimal in so far as it
   minimizes overall within set differences.
7. To create a stratified research design (something like a block-randomized
   experiment), we first need a distance matrix --- something that records the
   similarities/differences between each treated and each control unit. Last
   time we used (1) distances on a single variable and (2) we used a
   Mahalanobis distance to represent multivariate distance in a space of more
   than one covariate.

## Today: Propensity distances, exact matching, calipers, combining distance matrices

1. Another way to combine covariates is the propensity score.
2. When we have a categorical or binary covariate that is important sometimes
   we want to exactly stratify on it --- leading to exact matching.
3. Sometimes we want to restrict the possible matches --- and to allow the
   matching algorithm to exclude certain units from the research design
   entirely. This is the role of calipers.
4. We can combine distance matrices in order to make a strong argument about
   our research design.


## The World of Matching Today

This is an active and productive research area (i.e. lots of new ideas, not
everyone agrees about everything). Here are just a few of the different
approaches to creating strata or sets or selecting subsets. I'm excluding work
that mostly focuses on finding optimal weights directly rather than first
finding defensible/optimal stratified designs from this list, but that is also
an area worth exploring.

 - The work on cardinality matching and fine balance
   <http://jrzubizarreta.com/>
   <https://cran.rstudio.com/web/packages/designmatch/> )
 - The work on speedier approximate full matching with more data  <http://fredriksavje.com/>
   <https://github.com/fsavje/quickmatch>, <https://cran.r-project.org/web/packages/rcbalance/index.html>.
 - The work on using genetic algorithms to (1) find approximate strata
   **with-replacement** <http://sekhon.berkeley.edu/matching/> and (2) to find
   an approximation to a completely randomized study (i.e. best subset
   selection) <http://cho.pol.illinois.edu/wendy/papers/bossor.pdf>
 - The work on coarsened exact matching <https://gking.harvard.edu/cem>
 - The work on entropy-balancing approach to causal effects <https://web.stanford.edu/~jhain/software.htm#ebal>.
 - The covariate-balancing propensity score (CBPS) <https://imai.princeton.edu/research/CBPS.html>.



## Overly Ambitious Plan

  - 00:00 -- 00:30 --- Review
  - 00:30 -- 01:30 ---  Lecture by Jake with lots of questions from the class
  - 01:30 -- 01:40 --- Break
  - 01:40 -- 02:00 --- Questions about the lecture and/or readings
  - 02:00 -- 03:00 --- Break and Exercise 2 and maybe Exercise 3.
  - 03:00 -- 04:00 --- Discussion of questions arising from the exercises and
    questions lingering from reading, or lecture, or open discussion on any
    topic.

## "Controlling For" versus "Holding Constant"

These two phrases imply differ adjustments:

```{r}
load(url("http://jakebowers.org/Data/meddat.rda"))
meddat <- transform(meddat,
  HomRate03 = (HomCount2003 / Pop2003) * 1000,
  HomRate08 = (HomCount2008 / Pop2008) * 1000
)
```

#  Matching on Many Covariates: Using Propensity Scores


## The propensity score

Given covariates $\mathbf{x} (=(x_1, \ldots, x_k))$, and a
treatment variable $Z$, $Z(u) \in \{0, 1\}$,  $\PP (Z \vert \mathbf{x})$ is known as the (true) \textbf{propensity score} (PS).
$$ \phi( \mathbf{x} ) \equiv \log\left( \PP (Z=1 \vert \mathbf{x})/\PP (Z=0 \vert \mathbf{x}) \right)$$
is also known as the PS.  In practice, one works
with an estimated PS, $\hat{\PP} (Z \vert \mathbf{x})$ or
$\hat{\phi}(\mathbf{x})$.

Theoretically, propensity-score strata or matched sets both

 1. reduce extrapolation; and
 2. balance each of $x_1, \ldots, x_k$.

They do this by making the comparison more "experiment-like", at least in terms of $x_1, \ldots, x_k$.

Theory @rosrub83 also tells us that in the **absence of hidden bias**, such a stratification
supports unbiased estimation of treatment effects.


## Propensity scoring in practice

 - Fitted propensity scores help identify extrapolation.
 - In practice, stratification on $\hat{\phi}(\mathbf{x})$
helps balance each of $x_1, \ldots, x_k$ compared to no stratification.


There are \emph{lots of cases} in which adjustment with the propensity score alone fails to generate estimates that agree with those of randomized studies.

There are various reasons for this, starting with:

 - lots of observational studies that don't measure quite enough $x$es or the right $x$es or the right $x$es in the right way
 - **hidden biases** --- propensity scores address bias on measured variables, not unmeasured ones.

## Intuition about the propensity score

A propensity score is the output of a function of covariates as they relate to
$Z$ (the "treatment" or "intervention"). Why reduce the dimension of $\bm{x}$
in this way rather than, say, using Mahalanobis distance?

\medskip

Recall that an experiment breaks the relationship between $Z$ and $\bm{x}=\{ x_1,x_2,\ldots \}$ but not between $\bm{x}$ and $Y$ or $y_1,y_0$.

\includegraphics[width=.25\textwidth]{xyzdiagram.pdf}

```{r tikzarrows, eval=FALSE, include=FALSE, engine='tikz', engine.opts=list(template="icpsr-tikz2pdf.tex")}
\usetikzlibrary{arrows}
\begin{tikzcd}[ampersand replacement=\&, column sep=small]
  Z  \arrow[from=1-1,to=1-3] &                               & Y \\
  &   \mathbf{x} \arrow[from=2-2,to=1-1, "\text{0 if Z rand}"] \arrow[from=2-2,to=1-3] &
\end{tikzcd}
```

Making strata of units who are similar on the propensity score reduces (or removes)
the relationship between $Z$ and the relevant $\mathbf{x}$ within strata (either the
units have similar values for $\bm{x}$ or the particular $x$s which do not have a
strong (linear, additive) relationship with $Z$).

## Matching on the propensity score

**Make the score** (Note that we will be using `brglm` or `bayesglm` in the
future because of logit separation problems when the number of covariates
increases.)

```{r}
theglm <- glm(nhTrt ~ nhPopD + nhAboveHS, data = meddat, family = binomial(link = "logit"))
thepscore <- theglm$linear.predictor
thepscore01 <- predict(theglm, type = "response")
````

We tend to match on the linear predictor rather than the version required to
range only between 0 and 1. Recall how distance matrices required choices of
distance metrics? We don't want to categorize two observations as "close" just
because the logit function squashed them together near 0 or 1.

```{r echo=FALSE, out.width=".7\\textwidth"}
par(mfrow = c(1, 2), oma = rep(0, 4), mar = c(3, 3, 2, 0), mgp = c(1.5, .5, 0))
boxplot(split(thepscore, meddat$nhTrt), main = "Linear Predictor (XB)")
stripchart(split(thepscore, meddat$nhTrt), add = TRUE, vertical = TRUE)

boxplot(split(thepscore01, meddat$nhTrt), main = "Inverse Link Function (g^-1(XB))")
stripchart(split(thepscore01, meddat$nhTrt), add = TRUE, vertical = TRUE)
```


## Matching on the propensity score: What do the distance matrix entries mean?

optmatch creates a scaled propensity score distance by default --- scaling by, roughly, the pooled median absolute deviation of the covariate (or here, the propensity score). So, the distance matrix entries are like standard deviations  --- standardized scores.

```{r}
## Create a distance matrix using the propensity scores
psdist <- match_on(theglm, data = meddat)
psdist[1:4, 1:4]
```

What do those distances mean?
```{r}
simpdist <- outer(thepscore, thepscore, function(x, y) {
  abs(x - y)
})
mad(thepscore[meddat$nhTrt == 1])
mad(thepscore[meddat$nhTrt == 0])
(mad(thepscore[meddat$nhTrt == 1]) + mad(thepscore[meddat$nhTrt == 0])) / 2
## We can see the actual R function here: optmatch:::match_on_szn_scale
optmatch:::match_on_szn_scale(thepscore, Tx = meddat$nhTrt)
simpdist["101", c("401", "402", "403")]
simpdist["101", c("401", "402", "403")] / .9137
psdist["101", c("401", "402", "403")]
```


## Matching on the propensity score

The following design balances the two covariates used in the creation of the
propensity score well. It does not balance the baseline outcome well (not that
we assumed it would, but demonstrating here that the covariates used for the
creation of the design need not necessarily be all of those used to
**evaluate** the design).

```{r}
fmPs <- fullmatch(psdist, data = meddat)
summary(fmPs, min.controls = 0, max.controls = Inf)
meddat$fmPs <- factor(fmPs)

xb2 <- xBalance(nhTrt ~ nhPopD + nhAboveHS,
  strata = list(fmPs = ~fmPs), data = meddat,
  report = "all"
)
xb2$overall[, ]

xb2a <- xBalance(nhTrt ~ nhPopD + nhAboveHS + HomRate03,
  strata = list(fmPs = ~fmPs), data = meddat,
  report = "all"
)
xb2a$overall[, ]
```

Compare to Mahalanobis distance:

```{r}
mh_dist <- match_on(nhTrt ~ nhPopD + nhAboveHS,
  data = meddat,
  method = "rank_mahalanobis"
)
fmMh <- fullmatch(mh_dist, data = meddat)
summary(fmMh)
xb3 <- xBalance(nhTrt ~ nhPopD + nhAboveHS + HomRate03,
  strata = list(unstrat = NULL, fmPs = ~fmPs, fmMh = ~fmMh), data = meddat,
  report = "all"
)
xb3$overall[, ]
xb3$results[, "std.diff", ]
```

```{r}
plot(xb3, ggplot = TRUE) + theme_classic()
```

# Matching Tricks of the Trade: Calipers, Exact Matching

## Calipers

The optmatch package allows calipers (which forbids certain pairs from being matched).^[You can implement penalties by hand.] Here, for example, we forbid comparisons which differ by more than 2 propensity score standardized distances.

```{r}
## First inspect the distance matrix itself: how are the distances distributed?
quantile(as.vector(psdist), seq(0, 1, .1))
## Next, apply a caliper (setting entries to Infinite)
psdistCal <- psdist + caliper(psdist, 2)
as.matrix(psdist)[5:10, 5:10]
as.matrix(psdistCal)[5:10, 5:10]
```
## Calipers

The optmatch package allows calipers (which forbid certain pairs from being matched).^[You can implement penalties by hand.] Here, for example, we forbid comparisons which differ by more than 2 standard deviations on the propensity score. (Notice that we also use the `propensity.model` option to `summary` here to get a quick look at the balance test:)

```{r}
fmCal1 <- fullmatch(psdist + caliper(psdist, 2), data = meddat, tol = .00001)
summary(fmCal1, min.controls = 0, max.controls = Inf, propensity.model = theglm)
pmCal1 <- pairmatch(psdist + caliper(psdist, 2), data = meddat, remove.unmatchables = TRUE)
summary(pmCal1, propensity.model = theglm)
```

## Calipers

Another example: We may want to match on mahalanobis distance but disallow any pairs with extreme propensity distance and/or extreme differences in baseline homicide rates (here using many covariates all together).


```{r}
## Create an R formulate object from vectors of variable names
balfmla <- reformulate(c("nhPopD", "nhAboveHS"), response = "nhTrt")

## Create a mahalanobis distance matrix (of rank transformed data)
mhdist <- match_on(balfmla, data = meddat, method = "rank_mahalanobis")

## Now make a matrix recording absolute differences between neighborhoods in
## terms of baseline homicide rate
tmpHom03 <- meddat$HomRate03
names(tmpHom03) <- rownames(meddat)
absdist <- match_on(tmpHom03, z = meddat$nhTrt, data = meddat)
absdist[1:3, 1:3]
quantile(as.vector(absdist), seq(0, 1, .1))

## Now create a new distance matrix using two calipers:
distCal <- mhdist + caliper(psdist, 2) + caliper(absdist, 2)
as.matrix(distCal)[5:10, 5:10]
## Compare to:
as.matrix(mhdist)[5:10, 5:10]
```

## Calipers

Now, use this new matrix for the creation of stratified designs --- but possibly excluding some units (also showing here the `tol` argument. The version with the tighter tolerance produces a solution with smaller overall distances)

```{r}
fmCal2a <- fullmatch(distCal, data = meddat, tol = .9)
summary(fmCal2a, min.controls = 0, max.controls = Inf)
fmCal2b <- fullmatch(distCal, data = meddat, tol = .00001)
summary(fmCal2b, min.controls = 0, max.controls = Inf)

meddat$fmCal2a <- fmCal2a
meddat$fmCal2b <- fmCal2b

fmCal2a_dists <- matched.distances(fmCal2a, distCal)
fmCal2b_dists <- matched.distances(fmCal2b, distCal)

mean(unlist(fmCal2a_dists))
mean(unlist(fmCal2b_dists))
```

## Exact Matching

We often have covariates that are categorical/nominal and for which we really care about strong balance. One approach to solve this problem is match **exactly** on one or more of such covariates. If `fullmatch` or `match_on` is going slow, this is also an approach to speed things up.

```{r echo=FALSE}
meddat$classLowHi <- ifelse(meddat$nhClass %in% c(2, 3), "hi", "lo")
```

```{r}
dist2 <- mhdist + exactMatch(nhTrt ~ classLowHi, data = meddat)
## or mhdist <- match_on(balfmla,within=exactMatch(nhTrt~classLowHi,data=meddat),data=meddat,method="rank_mahalanobis")
## or fmEx1 <- fullmatch(update(balfmla,.~.+strata(classLowHi)),data=meddat,method="rank_mahalanobis")
fmEx1 <- fullmatch(dist2, data = meddat, tol = .00001)
summary(fmEx1, min.controls = 0, max.controls = Inf)
print(fmEx1, grouped = T)
meddat$fmEx1 <- fmEx1
```
## Exact Matching

```{r}
ftable(Class = meddat$classLowHi, Trt = meddat$nhTrt, fmEx1, col.vars = c("Class", "Trt"))
```

## What about using many covariates? The separation problem in logistic regression

What if we want to match on more than two covariates? Let's step through the following to discover a problem with logistic regression when the number of covariates is large relative to the size of the dataset.

```{r echo=TRUE}
library(splines)
library(arm)
thecovs <- unique(c(names(meddat)[c(5:7,9:24)],"HomRate03"))
balfmla<-reformulate(thecovs,response="nhTrt")
psfmla <- update(balfmla,.~.+ns(HomRate03,2)+ns(nhPopD,2)+ns(nhHS,2))
glm0 <- glm(balfmla,data=meddat,family=binomial(link="logit"))
glm1 <- glm(psfmla,data=meddat,family=binomial(link="logit"))
bayesglm0 <- bayesglm(balfmla,data=meddat,family=binomial(link="logit"))
bayesglm1 <- bayesglm(psfmla,data=meddat,family=binomial(link="logit"))
psg1 <- predict(glm1,type="response")
psg0 <- predict(glm0,type="response")
psb1 <- predict(bayesglm1,type="response")
psb0 <- predict(bayesglm0,type="response")
```
## The separation problem

Logistic regression is excellent at discriminating between groups \ldots often **too excellent** for us \autocite{gelman2008weakly}. First evidence of this is big and/or missing coefficients in the propensity score model. See the coefficients below (recall that we are predicting `nhTrt` with these covariates in those models):

```{r echo=FALSE}
thecoefs <- rbind(glm0=coef(glm0)[1:20],
      glm1=coef(glm1)[1:20],
      bayesglm0=coef(bayesglm0)[1:20],
      bayesglm1=coef(bayesglm1)[1:20]
      )
thecoefs[,1:5]
```

## The separation problem

```{r, echo=FALSE, out.width=".9\\textwidth"}
par(mfrow=c(1,2))
matplot(t(thecoefs),axes=FALSE)
axis(2)
axis(1,at=0:19,labels=colnames(thecoefs),las=2)

matplot(t(thecoefs),axes=FALSE,ylim=c(-15,10))
axis(2)
axis(1,at=0:19,labels=colnames(thecoefs),las=2)

legend("topright",col=1:4,lty=1:4,legend=c("glm0","glm1","bayesglm0","bayesglm1"))
```


## The separation problem in logistic regression

So, if we are interested in using the propensity score to compare observations in regards the multi-dimensional space of many covariates, we would probably prefer a dimensional reduction model like `bayesglm` over `glm`.

```{r out.width=".9\\textwidth", echo=FALSE}
par(mfrow=c(2,2),mar=c(3,3,2,.1))
boxplot(psg0~meddat$nhTrt,main=paste("Logit",length(coef(glm0))," parms",sep=" "))
stripchart(psg0~meddat$nhTrt,vertical=TRUE,add=TRUE)
boxplot(psg1~meddat$nhTrt,main=paste("Logit",length(coef(glm1))," parms",sep=" "))
stripchart(psg1~meddat$nhTrt,vertical=TRUE,add=TRUE)
boxplot(psb0~meddat$nhTrt,main=paste("Shrinkage Logit",length(coef(bayesglm0))," parms",sep=" "))
stripchart(psb0~meddat$nhTrt,vertical=TRUE,add=TRUE)
boxplot(psb1~meddat$nhTrt,main=paste("Shrinkage Logit",length(coef(bayesglm1))," parms",sep=" "))
stripchart(psb1~meddat$nhTrt,vertical=TRUE,add=TRUE)
```

# How to find a good matched design?

## Searching for a good matched design

Often we find ourselves hunting for a matched design by fiddling with different
parameters, etc.. Here I show a couple of approaches that are a bit more
systematic or at least may help organize the fiddling.

## Design Search for both precision and balance

Here I demonstrate searching for two calipers.

```{r gridsearch, cache=FALSE}
findbalance<-function(x,mhdist=mhdist,psdist=psdist,thedat=meddat){
    ##message(paste(x,collapse=" "))
    thefm<-try(fullmatch(psdist+caliper(mhdist,x[2])+caliper(psdist,x[1]),data=thedat,tol=.00001))

    if(inherits(thefm,"try-error")){
        return(c(x=x,d2p=NA,maxHR03diff=NA,n=NA,effn=NA))
    }

    thedat$thefm <- thefm

    thexb<-try(xBalance(balfmla,strata=list(thefm=~thefm),
            data=thedat,
            report=c("chisquare.test","p.values")),silent=TRUE)

    if(inherits(thexb,"try-error")){
        return(c(x=x,d2p=NA,maxHR03diff=NA,n=NA,effn=NA))
    }

    maxHomRate03diff<-max(unlist(matched.distances(thefm,distance=absdist)))

    return(c(x=x,d2p=thexb$overall["thefm","p.value"],
            maxHR03diff=maxHomRate03diff,
            n=sum(!is.na(thefm)),
            effn=summary(thefm)$effective.sample.size))

}

```

## Design Search for both precision and balance

```{r eval=TRUE,echo=FALSE, cache=TRUE, warning=FALSE}
## Test the function
findbalance(c(3,3),thedat=meddat,psdist=psdist,mhdist=mhdist)
## Don't worry about errors for certain combinations of parameters
maxmhdist<-max(as.vector(mhdist))
minmhdist<-min(as.vector(mhdist))
maxpsdist<-max(as.vector(psdist))
minpsdist<-min(as.vector(psdist))
```

```{r findbal, eval=FALSE}
set.seed(123455)
system.time({
    results<-replicate(1000,findbalance(x=c(runif(1,minpsdist,maxpsdist),
                runif(1,minmhdist,maxmhdist)),thedat=meddat,psdist=psdist,mhdist=mhdist))
}
)
```

```{r findbalpar, eval=TRUE, cache=TRUE, echo=FALSE}
## If you have a mac or linux machine you can speed this up:
library(parallel)
system.time({
    resultsList<-mclapply(1:1000,function(i){
        findbalance(x=c(runif(1,minpsdist,maxpsdist),
                runif(1,minmhdist,maxmhdist)),thedat=meddat,psdist=psdist,mhdist=mhdist)
                },
        mc.cores=detectCores())
    resultsListNA<-sapply(resultsList,function(x){ any(is.na(x)) })
    results<-simplify2array(resultsList[!resultsListNA])
}
)

```


## Which matched design might we prefer?

Now, how might we interpret the results of this search for matched designs?
Here are a few ideas.

```{r }
if(class(results)=="list"){
    resAnyNA<-sapply(results,function(x){ any(is.na(x)) })
    resNoNA<-simplify2array(results[!resAnyNA])
} else {
    resAnyNA<-apply(results,2,function(x){ any(is.na(x)) })
    resNoNA<-simplify2array(results[,!resAnyNA])
}
apply(resNoNA,1,summary)
highbalres<-resNoNA[,resNoNA["d2p",]>.5]
apply(highbalres,1,summary)
```

## Which matched design might we prefer?

The darker points have smaller maximum within set differences on the baseline outcome.

```{r eval=TRUE, echo=FALSE}
# color points more dark for smaller differences
plot(resNoNA["d2p",],resNoNA["n",],
    xlab='d2p',ylab='n',
    col=gray(1- ( resNoNA["maxHR03diff",]/max(resNoNA["maxHR03diff",]))),
    pch=19)

## identify(resNoNA["d2p",],resNoNA["n",],labels=round(resNoNA["maxHR03diff",],3),cex=.7)
```

## Which matched design might we prefer?

```{r canddesigns, eval=TRUE,echo=TRUE}
interestingDesigns<- (resNoNA["d2p",]>.4 & resNoNA["n",]>=20 &
    resNoNA["maxHR03diff",]<=3 & resNoNA["effn",] > 6)
candDesigns <- resNoNA[,interestingDesigns,drop=FALSE]
str(candDesigns)
apply(candDesigns,1,summary)
candDesigns<-candDesigns[,order(candDesigns["d2p",],decreasing=TRUE)]
candDesigns <- candDesigns[,1]
```

## How would we use this information in `fullmatch`?

```{r bigmatch}
stopifnot(nrow(candDesigns)==1)
fm4<-fullmatch(psdist+caliper(psdist,candDesigns["x1"])+caliper(mhdist,candDesigns["x2"]),data=meddat,tol=.00001)

summary(fm4,min.controls=0,max.controls=Inf,propensity.model=bayesglm0)

meddat$fm4<-NULL ## this line exists to prevent confusion with new fm4 objects
meddat[names(fm4),"fm4"]<-fm4

xb3<-xBalance(balfmla,strata=list(fm4=~fm4),
	      data=meddat, report=c("all"))
xb3$overall[,1:3]
zapsmall(xb3$results["HomRate03",,])
```

## Another approach: more fine tuned optimization

Here is another approach that tries to avoid searching the whole space. It focuses on getting close to a target $p$-value from the omnibus/overall balance test. Here we are just looking for one caliper value that gets us close to a particular target balance using one distance matrix.

```{r eval=TRUE,cache=FALSE}
matchAndBalance2<-function(x,distmat,alpha){
	#x is a caliper widths
	if(x>max(as.vector(distmat)) | x<min(as.vector(distmat))){ return(99999) }
	thefm<-fullmatch(distmat+caliper(distmat,x),data=meddat,tol=.00001)
	thexb<-xBalance(balfmla,
			strata=data.frame(thefm=thefm),
			data=meddat,
			report=c("chisquare.test"))
	return(thexb$overall[,"p.value"])
}

maxpfn<-function(x,distmat,alpha){
	## here x is the targeted caliper width and x2 is the next wider
	## caliper width
	p1<-matchAndBalance2(x=x[1],distmat,alpha)
	p2<-matchAndBalance2(x=x[2],distmat,alpha)
	return(abs( max(p1,p2) - alpha) )
}

maxpfn(c(minpsdist,minpsdist+1),distmat=psdist,alpha=.25)
#quantile(as.vector(psdist),seq(0,1,.1))
#sort(as.vector(psdist))[1:10]
```

## Another approach: more fine tuned optimization

```{r solnp, warning=FALSE, message=FALSE, cache=TRUE}
library(Rsolnp)
### This takes a long time
results3<-gosolnp(fun=maxpfn,
		  ineqfun=function(x,distmat,alpha){ x[2] - x[1] },
		  ineqLB = 0,
		  ineqUB = maxpsdist,
		  LB=c(minpsdist,minpsdist+.01),
		  UB=c(maxpsdist-.01,maxpsdist),
		  n.restarts=2,
		  alpha=.5,
          distmat=psdist,
		  n.sim=500,
		  rseed=12345,
		  control=list(trace=1)
)

results3$pars
results3$values
```

## Another approach: more fine tuned optimization

```{r}
maxpfn(results3$pars,distmat=psdist,alpha=.25)
matchAndBalance2(results3$pars[1],distmat=psdist,alpha=.25)
```

## Back to matching on a scalar: Inspecting the Design from a substantive perspective

Do any of these sets look like the differences are too big within the set?

```{r}
absHomMatch <- fullmatch(absdist, data = meddat)
meddat$absHomMatch <- absHomMatch
setdiffsHM <- meddat %>%
  group_by(absHomMatch) %>%
  summarize(
    mndiffs =
      mean(HomRate03[nhTrt == 1]) -
        mean(HomRate03[nhTrt == 0]),
    mnHomRate03 = mean(HomRate03),
    minHomRate03 = min(HomRate03),
    maxHomRate03 = max(HomRate03)
  )
setdiffsHM
```

## Decision Points in Creating Matched Designs

 - Which covariates and their scaling and coding. (For example, exclude covariates with no variation!)
 - Which distance matrices (scalar distances for one or two important variables, Mahalanobis distances (rank  transformed or not), Propensity distances (using linear predictors)).
 - (Possibly) which calipers (and how many, if any, observations to drop. Note about ATT as a random quantity and ATE/ACE as fixed.)
 - (Possibly) which exact matching or strata
 - (Possibly) which structure of sets (how many treated per control, how many controls per treated)
 - Which remaining differences are tolerable from a substantive perspective?
 - How well does the resulting research design compare to an equivalent block-randomized study (`xBalance`)?
 - (Possibly) How much statistical power does this design provide for the quantity of interest?
 - Other questions to ask about a research design aiming to help clarify comparisons.



## Next time:

 - Matching when we have more than one group (non-bipartite matching)

## Remaining questions?


## References

