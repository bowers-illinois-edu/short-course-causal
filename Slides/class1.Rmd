---
title: |
  | Matching for Adjustment and Causal Inference
  | Class 1: Experiments, Potential Outcomes, and Treatment Effects
date: '`r format(Sys.Date(), "%B %d, %Y")`'
author: Jake Bowers
bibliography:
 - ../BIB/references.bib
fontsize: 10pt
geometry: margin=1in
graphics: yes
biblio-style: authoryear-comp
biblatexoptions:
  - natbib=true
output:
  beamer_presentation:
    slide_level: 2
    keep_tex: true
    latex_engine: lualatex
    citation_package: biblatex
    incremental: true
    template: icpsr.beamer
    includes:
        in_header:
           - defs-all.sty
    md_extensions: +raw_attribute-tex_math_single_backslash+autolink_bare_uris+ascii_identifiers+tex_math_dollars
header-includes:
  - \setbeameroption{hide notes}
---

<!-- To show notes  -->
<!-- https://stackoverflow.com/questions/44906264/add-speaker-notes-to-beamer-presentations-using-rmarkdown -->

```{r echo=FALSE, include=FALSE, cache=FALSE}
library(here)
source(here::here("Exercises/rmd_setup.R"))
library(tidyverse)
library(optmatch)
library(RItools)
```

# Overview and Review

## Experiments, Potential Outcomes, and Treatment Effects

  1. Causal inference in randomized experiments and the idea of only partially
     observed **potential outcomes**
  2. Statistical inference for causal effects in randomized experiments via the
     Fisher and Neyman approaches \autocite[Chap 2]{rosenbaum2010},
     \autocite[Chap 1-3]{gerbergreen2012}: Estimation, Estimators, Tests, Testing.
  3. Why are randomized experiments special?
  4. What does "controlling for" do in a linear model when we do not have a
     randomized experiment? How can we make the case that we are "controlling
     for" enough?

## Overly Ambitious Plan

  - 00:00 -- 00:30  ---  Introductions: Name, Affiliation/Organization, Interest
    (Substantive or Methodological). Be thinking about (for later) a project
    that you are working on that involves some form of comparison of
    intervention with outcome, perhaps involving some causal inference.
  - 00:30 -- 01:00 ---  Lecture by Jake to introduce concepts of potential outcomes, treatment effects, unbiased estimation, and encourage
    questions and answers.
  - 01:00 -- 01:10 --- Break
  - 01:10 -- 01:30 --- Questions about the lecture and/or readings
  - 01:30 -- 02:00 --- Tell us about a project and/or data set that you might want to use in this class to practice and learn.
  - 02:00 -- 03:00 --- Exercise 1: Describe your data.
  - 03:00 -- 03:30 --- Jake discuss the problem of adjustment in observational studies as compared to experiments
  - 03:30 -- 04:00 --- Open Discussion on any topic.

## Notation and Concepts for Counterfactual Causal Inference

  - *Treatment* or *Intevention*  $Z_i=1$ for treatment and $Z_i=0$ for control
    for units $i$. (We mostly assume that all units **could have** $Z_i=1$ or
    $Z_i=0$. That it is not impossible for any unit to have either value.) (Q:
    What is a unit? Examples of interventions?)

  - Each unit has a pair of *potential outcomes* $(y_{i,Z_i=1},y_{i,Z_i=0})$
    (also written  $(y_{i,1},y_{i,0})$ ) (given SUTVA).
    - Without the SUTVA assumption, and with 4 units, with two having $Z_i=1$, unit $i=1$ would have the following potential outcomes: $(y_{i,1100},y_{i,1010},y_{i,1001},y_{i,0101},y_{i,0011})$
  - *Causal Effect under SUTVA*  when $y_{i,1} \ne y_{i,0}$, $\tau_i   =
    f(y_{i,1},y_{i,0})$ ex. $\tau_i =  y_{i,1} - y_{i,0}$. (Examples of Interfering Units and Not Interfering Units)

  - *Fundamental Problem of (Counterfactual) Causality* We only see one
    potential outcome $Y_i = Z_i * y_{i,1} + (1-Z_i) y_{i,0}$ (Examples?)

  - *Covariates*,  $\bX=\begin{bmatrix} x_{11} & \ldots & x_{1k} \\ \vdots &
    \vdots & \vdots \\  x_{n1} & \ldots & x_{nk} \end{bmatrix}$ is a matrix
    containing  background information about the units that might predict
    $(y_{i,1},y_{i,0})$ or $Z$ (but that don't predict $Z$ if $Z$ is randomized
    as in an experiment).

## The link between observation and learning about causal claims

We can learn about causal claims by observing [@brady2008cae]:

- **Persistent association** ``We always/mostly see $Y=1$ when $X=1$ and $Y=0$ when $X=0$.''
- **Counterfactual Difference** ``If  $X$ had not been this value, then $Y$ would not have been that value.''
- **Difference after manipulation** ``When we change $X$ from one value to another value, then $Y$ changes from one value to another value.'' (establishes causal priority of $X$ over $Y$, implied that $Y$ would not have changed.).
- **Difference after operation of a mechanism** ``Once upon a time $A$ changed $X$, and then one day $X$ changed $B$, and because of that $B$ changed $C$, and finally $C$ changed $Y$.''


## How to interpret "X causes Y"?

 -  We can establish that X causes Y without knowing the mechanism. The mechanism
    can be complex, and it can involve probability: X causes Y sometimes because of
A and sometimes because of B.
 - "X causes Y" can mean "With X, probability of Y is higher than would be
   without X." or "Without X there is no Y." Either is compatible with the
counterfactual idea.
 - Correlation is not causation: Favorite examples?
 - "X causes Y" is a statement about what didn't happen: "If X had not
   operated, occurred, then Y would not have occurred." (More about the
fundamental problem of counterfactual causation later)

## Example: Honey and Colds

- Your friend says that eating raw honey (a traditional remedy) reduces the duration of colds.
- Say we observe the duration of a cold among people who took honey and the duration of a cold among people who did not and we saw that the duration was shorter among those taking honey. What kinds of alternative explanations might we come up with for this result (other than the idea that honey shortened the duration of the cold)?
- How might a randomized design help us here?

## Example: What does randomization provide?

```{r, echo=FALSE, results="hide"}
set.seed(12345)
honeydat <- data.frame(x1=c(1,1,2,3,1,0,0,0,0,2),
    x2=sample(1:10,size=10,replace=TRUE))
honeydat$Zlat <- with(honeydat,.5*x1 + runif(10))
honeydat$Z <- as.numeric(honeydat$Z > median(honeydat$Z))
## honeydat$y0 <- with(honeydat,x1+x2+Z+rpois(10,lambda=5))
honeydat$y0 <- with(honeydat, x1 + .25*x1^2+rpois(10,lambda=8))
summary(lm(y0~x1+x2,data=honeydat))
## cor(honeydat[,c("x1","x2","Z","y0")])
honeydat$Zlat <- NULL
honeydat$y1 <- with(honeydat,y0 - c(1,1,2,3,1,0,0,0,0,2))
honeydat$Y <- with(honeydat,y1*Z + y0*(1-Z))
with(honeydat,mean(Y[Z==1]) - mean(Y[Z==0]))
honeydat$tau <- honeydat$y1 - honeydat$y0
honeydat$id <- 1:10
dat <- honeydat[,c("id","x1","y0","y1","tau")]
```

Imagine these were the underlying potential outcomes and that x1 -> y0.

```{r}
kable(dat)
message("Average (additive) causal effect is: ", with(honeydat, mean(y1-y0)))
```

Let us run a randomized experiment and see how we do:

## An RCT:

What is happening here? How would we know whether `complete_ra` worked as it should?

```{r, echo=TRUE}
library(randomizr)
set.seed(12345)
dat$Z <- randomizr::complete_ra(N=10,m=5)
dat$Y <- with(dat,Z*y1 + (1-Z)*y0)
kable(dat)
```


## Estimating the ATE in an RCT:

Here are two proposals for estimating the ATE. How would we know whether either
or both of them work well? (What do we want estimators to do for us?)

```{r echo=TRUE}
est1 <- function(Z,Y){
    mean(Y[Z==1]) - mean(Y[Z==0])
}
est2 <- function(Z,Y){
    coef(lm(Y~Z))[["Z"]]
}

with(dat,est1(Z=Z,Y=Y))
with(dat,est2(Z=Z,Y=Y))
```

## How does randomization help us trust our estimators?

```{r, echo=TRUE}
## The truth:
with(dat,mean(y1-y0))

new_exp <- function(Z){
    ## This next shuffles Z
    newZ <- sample(Z)
    return(newZ)}

new_est <- function(newZ,y0,y1,the_est){
    newY <- newZ * y1 + (1-newZ)*y0
    result <- the_est(Z=newZ,Y=newY)
}

set.seed(1235)
dist_est1 <- with(dat, replicate(100, new_est(newZ=new_exp(Z),y0=y0,y1=y1,the_est=est1)))
mean(dist_est1)
dist_est1 <- with(dat, replicate(1000, new_est(newZ=new_exp(Z),y0=y0,y1=y1,the_est=est1)))
mean(dist_est1)
```

## How does randomization help us trust our estimators?

What did randomization provide here? (1) Grounds for repetition (i.e. we **knew**
how to repeat the generation of $Z$), (2) No need to mention `x1`, (3) an
unbiased estimator.

## What about if we didn't know how Z was assigned?

Imagine that `x1` causes `Z`.

```{r echo=TRUE}
dat$Z <- as.numeric(dat$x1 > 0)
with(dat,mean(y1-y0))
est1(dat$Z,dat$Y)
```

How would we assess bias if we didn't know that x1 caused Z?
 - We cannot simply shuffle Z. Because we don't know how Z arose.
 - Hmm... do we know how x1 was generated? We could re-generate x1 and *hope* that our x1 to Z function is right
 - We could repeatedly re-generate Y itself if we **knew** how it was created.
 - We could resample the entire dataset if we **knew** how it was sampled.

## What about controlling for x1?

If we knew that x1 causes both Z and Y, then perhaps we can "control for it":

```{r echo=TRUE}

est2 <- lm(Y~Z+x1,data=dat)
coef(est2)[["Z"]]

```

Is this enough? Did we do a good job? How would we know? What does "controlling for" really mean?


# Summary and Overview

## Benefits of Randomized Designs

Randomization makes $y_1, y_0, \bX \perp Z$. How to make use of this fact in a randomized experiment?

 1. Interpretable comparisons (lack of omitted variable bias, confounding,
    selection bias)
      - Can I interpret differences in outcome as caused by $Z$ and not $X$?
	Is it easy to confuse the effect of $Z$  with the  effects of $X$?
      - How does  randomization do this? How  does randomization eliminate
	**alternative explanations**?
 2. Reliable statistical inferences (estimators and tests)
      - The idea of **design-based** versus **model-based** statistical
	inference (next few slides).


## Design Based Approach 1: Test Hypotheses

 1. Make a guess about $\tau_i$.
 2. Then measure surprise or consistency of data with this guess given the
    design. (Given all of the ways this experiment could have  occurred, how
    many look more extreme than what we observe? Does our observation look
    typical or rare?)

\centering
  \includegraphics[width=.6\textwidth]{images/cartoonFisherNew1.pdf}

## Design Based Approach 1: Test Hypotheses

\centering
  \includegraphics[width=\textwidth]{images/cartoonFisherNew1.pdf}

## Design Based Approach 1: Test Hypotheses

\centering
  \includegraphics[width=\textwidth]{images/cartoonFisherNew2.pdf}



## Design Based Approach 2: Estimate Averages

  1. Notice that the observed $Y_i$ are a sample from  the (small, finite) population of $(y_{i,1},y_{i,0})$.
  2. Decide to focus on the average, $\bar{\tau}$, because sample averages, $\hat{\bar{\tau}}$ are unbiased and consistent estimators of population averages.
  3. Estimate $\bar{\tau}$ with the observed difference in means.

\centering
  \includegraphics[width=.5\textwidth]{images/cartoonNeyman.pdf}

## Design Based Approach 2: Estimate Averages

\centering
  \includegraphics[width=.9\textwidth]{images/cartoonNeyman.pdf}


## Approaches to creating interpretable comparisons:

   - Randomized experiments (more precision from reducing heterogeneity in $Y$)
   - Instrumental variables (with randomized $Z$ created $D$)
   - Natural Experiments / Discontinuities (one $X$ creates $Z$) (includes RDD)
   - Difference-in-Differences (reduce bias *and* increase precision from reducing heterogeneity in $Y$)
   - Semi-/Non-parametric Covariance Adjustment (ex. Matching)
   - Parametric covariance adjustment

## Lingering Questions?


## References

