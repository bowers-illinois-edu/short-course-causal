---
title: |
  | Matching for Adjustment and Causal Inference
  | Class 1: Experiments, Potential Outcomes, and Treatment Effects
date: '`r format(Sys.Date(), "%B %d, %Y")`'
author: Jake Bowers
bibliography:
 - ../BIB/references.bib
fontsize: 10pt
geometry: margin=1in
graphics: yes
biblio-style: authoryear-comp
biblatexoptions:
  - natbib=true
output:
  beamer_presentation:
    slide_level: 2
    keep_tex: true
    latex_engine: lualatex
    citation_package: biblatex
    incremental: false
    template: icpsr.beamer
    includes:
        in_header:
           - defs-all.sty
    md_extensions: +raw_attribute-tex_math_single_backslash+autolink_bare_uris+ascii_identifiers+tex_math_dollars
    pandoc_args: [ "--csl", "chicago-author-date.csl" ]
colorlinks: true
header-includes:
  - \setbeameroption{hide notes}
---

<!-- To show notes  -->
<!-- https://stackoverflow.com/questions/44906264/add-speaker-notes-to-beamer-presentations-using-rmarkdown -->

```{r echo=FALSE, include=FALSE, cache=FALSE}
library(here)
source(here::here("Exercises/rmd_setup.R"))
library(tidyverse)
library(optmatch)
library(RItools)
library(estimatr)
library(DeclareDesign)
library(coin)
library(kableExtra)
```

# Overview and Review

## Overview of the class {.shrink}

**At the end:**

- We estimate the TV ad caused an increase in vaccinations by 5 pct points and
  that it would be surprising to see this size of effect if the ad had no
  effect ($p=.001$).
- Recall that we compare pairs of people with the same age, so differences in
  age cannot explain the difference.
- Any differences within pair in income are smaller than we would see in an
  equivalent pair-randomized experiment.
- Also, although we did not measure party ID, and surely Republicans and
  Democrats of the same age are not equally likely to watch the ads,
  Republicans have to be 5x more likely to watch the ad than Democrats (of the
  same age) before we would estimate an effect of 0 or $p=.051$.

See also \autocite{rabb2022pnas}.

## Overview of the class {.shrink}

- We **infer** about counterfactual causal mechanisms by **estimating** and
  **testing**: *Causal inference (in this class) requires statistical
  inference.*
- Statistical inference requires **distributions** (we cannot know whether an
  estimator is unbiased without an idea of a distribution, we cannot calculate
  a $p$-value without a distribution, etc.)
- Distributions require **thought experiments** about repetition: We *imagine*
  that we could (a) re-draw random samples from a the same population or (b)
  re-assign randomized treatment. (We can also imagine reusing our priors and
  likelihood.)
- We will use the fact that distributions of certain test statistics is **known
  for randomized experiments** to (a) reason about whether we have a good
  non-randomized research design, (b) analyze outcomes, and (c) reason about
  sensitivity of results fo unobserved confounders.


So: We will start with randomized experiments even though the rest of the
course is about non-randomized studies.


## Experiments, Potential Outcomes, and Treatment Effects

  1. Causal inference in randomized experiments and the idea of only partially
     observed **potential outcomes**. The idea that we can use what we observe
     to learn about what we cannot observe.
  2. Statistical inference for causal effects in randomized experiments via the
     Fisher and Neyman approaches \autocite[Chap 2]{rosenbaum2010},
     \autocite[Chap 1-3]{gerbergreen2012}: Estimation, Estimators, Tests,
     Testing.
  3. Why are randomized experiments special? Unbiased estimators. The ability
     to assess bias. The ability to exclude alternative explanations. Tests
     with known error rates. The ability to assess error rates of tests.
  4. What does "controlling for" do in a linear model when we do not have a
     randomized experiment? How can we make the case that we are "controlling
     for" enough?

Note: You can download (and contribute to) course materials at \href{https://github.com/bowers-illinois-edu/short-course-causal}{https://github.com/bowers-illinois-edu/short-course-causal}

## Overly Ambitious Plan

 - Introduction to Jake
 - Introduction to the idea of the course: roadmap
 - Introductions to you and a project that you might work on during the week.
 - Jake introduces concepts:  potential outcomes, treatment effects, unbiased estimation, and encourages questions and answers.
 - Coffee Break
 - Questions about the lecture
 - Exercise 1: Describe your data (and update R).
 - Break
 - Jake discuss the problem of adjustment in observational studies as compared to experiments
 - Open Discussion and/or work on projects

# Concepts and Notation for Causal Inference and Statistical Inference

## Notation and Concepts for Counterfactual Causal Inference

  - *Treatment* or *Intevention*  $Z_i=1$ for treatment and $Z_i=0$ for control
    for units $i$. (We mostly assume that all units **could have** $Z_i=1$ or
    $Z_i=0$. That it is not impossible for any unit to have either value.) (Q:
    What is a unit? Examples of interventions?)

  - Each unit has a pair of *potential outcomes* $(y_{i,Z_i=1},y_{i,Z_i=0})$
    (also written  $(y_{i,1},y_{i,0})$ ) (given SUTVA).
    - Without the SUTVA assumption, and with 4 units, with two having $Z_i=1$, unit $i=1$ would have the following potential outcomes: $(y_{i,1100},y_{i,1010},y_{i,1001},y_{i,0101},y_{i,0011})$
  - *Causal Effect under SUTVA*  when $y_{i,1} \ne y_{i,0}$, $\tau_i   =
    f(y_{i,1},y_{i,0})$ ex. $\tau_i =  y_{i,1} - y_{i,0}$. (Examples of Interfering Units and Not Interfering Units)

  - *Fundamental Problem of (Counterfactual) Causality* We only see one
    potential outcome $Y_i = Z_i * y_{i,1} + (1-Z_i) y_{i,0}$ (Examples?)

  - *Covariates*,  $\bX=\begin{bmatrix} x_{11} & \ldots & x_{1k} \\ \vdots &
    \vdots & \vdots \\  x_{n1} & \ldots & x_{nk} \end{bmatrix}$ is a matrix
    containing  background information about the units that might predict
    $(y_{i,1},y_{i,0})$ or $Z$ (but that don't predict $Z$ if $Z$ is randomized
    as in an experiment).

## The observation and unobserved causal comparisons

We can learn about unobserved but theorized causal mechanisms by observing the world [@brady2008cae]:

- **Persistent association** "We always/mostly see $Y=1$ when $X=1$ and $Y=0$
  when $X=0$."
- **Counterfactual Difference** "If  $X$ had not been this value, then $Y$
  would not have been that value."
- **Difference after manipulation** "When we change $X$ from one value to
  another value, then $Y$ changes from one value to another value."
  (establishes causal priority of $X$ over $Y$, implied that $Y$ would not have
  changed.).
- **Difference after operation of a mechanism** "Once upon a time $A$ changed
  $X$, and then one day $X$ changed $B$, and because of that $B$ changed $C$,
  and finally $C$ changed $Y$."

All approaches are useful. This week we are focusing on the counterfactual approach and somewhat on the persistent association.

## How to interpret "X causes Y" in counterfactual terms?

The counterfactual approach requires that we can imagine units with and without $X$ (or say, $X=1$ and $X=0$).

 -  We can establish that X causes Y without knowing the mechanism. The
   mechanism can be complex, and it can involve probability: X causes Y
   sometimes because of A and sometimes because of B.
 - "X causes Y" can mean "With X, probability of Y is higher than would be
   without X." or "Without X there is no Y." Either is compatible with the
   counterfactual idea.
 - Of course: Correlation is not causation.
 - "X causes Y" is a statement about *what didn't happen*: "If X had not
   operated, occurred, then Y would not have occurred." (More about the
   fundamental problem of counterfactual causation later)

Today: Show that a randomized experiment allows us to **learn about**
unobserved counterfactual outcomes using statistical inference.

```{r, echo=FALSE, results="hide"}
set.seed(12345)
honeydat <- data.frame(x1=c(1,1,2,3,1,0,0,0,0,2),
    x2=sample(1:9,size=10,replace=TRUE))
honeydat$Zlat <- with(honeydat,.5*x1 + runif(10))
honeydat$Z <- as.numeric(honeydat$Z > median(honeydat$Z))
## honeydat$y0 <- with(honeydat,x1+x2+Z+rpois(10,lambda=5))
honeydat$y0 <- with(honeydat, x1 + .25*x1^2+rpois(10,lambda=8))
summary(lm(y0~x1+x2,data=honeydat))
## cor(honeydat[,c("x1","x2","Z","y0")])
honeydat$Zlat <- NULL
honeydat$y1 <- with(honeydat,y0 - c(1,1,2,3,1,0,0,0,0,2))
honeydat$Y <- with(honeydat,y1*Z + y0*(1-Z))
with(honeydat,mean(Y[Z==1]) - mean(Y[Z==0]))
honeydat$tau <- honeydat$y1 - honeydat$y0
honeydat$id <- 1:10
dat <- honeydat[,c("id","x1","x2","y0","y1","tau")]
```


## Example: Honey and Colds {.shrink}

Your friend explains a causal mechanism that eating raw honey reduces the
duration of colds. What kinds of **alternative** explanations might we come up
with for this result?

Imagine these were the underlying potential outcomes with $x_1$ and $x_2$
representing two of those explanations and that $x_1 \rightarrow y_0$ and that
$x_2 \not \rightarrow y_0$.

```{r}
kable(dat)
```

The true, unobserved, average (additive) causal effect is: `r with(honeydat, mean(y1-y0))`.

Let us run a randomized experiment and see how we do:

## An RCT:

What is happening here? How would we know whether `complete_ra` worked as it should?

```{r, echo=TRUE}
library(randomizr)
set.seed(12345)
dat$Z <- complete_ra(N=10,m=5)
dat$Y <- with(dat,Z*y1 + (1-Z)*y0)
kable(dat)
```


## Assessing randomization {.allowframebreaks}

We expect that the distributions of $x_1$ and $x_2$ would be (nearly) the same between the treated and control groups. We write "0" below, but in fact, randomization does not make those relationships exactly 0.

\begin{center}
\begin{tikzcd}[column sep=large]
	  Z  \arrow[from=1-1,to=1-4] &    &    & Y \\
	   x_1 \arrow[from=2-1,to=1-1, "\text{0 if $Z$ is randomized}"] \arrow[from=2-1,to=1-4] & & &
	   x_2 \arrow[from=2-4,to=1-1, "\text{0 if $Z$ is randomized}",sloped] \arrow[from=2-4,to=1-4]
\end{tikzcd}
\end{center}

Here, just looking at means:
```{r, echo=TRUE}
dat %>% group_by(Z) %>% reframe(mean(x1),mean(x2))
```

```{r, echo=TRUE}
library(RItools)
bal1 <- balanceTest(Z~x1+x2,data=dat)
bal1$results[,1:4,]
```

## What **should** we expect from an experiment?

...in regards covariate balance? Lets simulate to learn:

```{r, echo=TRUE}

new_exp <- function(Z){
	newZ <- sample(Z)
	return(newZ)
}

diff_means <- function(x,Z){
	mean(x[Z==1]) - mean(x[Z==0])
}

all_cov_bal<- replicate(1000,diff_means(x=dat$x1,Z=new_exp(dat$Z)))

summary(all_cov_bal)

obs_cov_bal <- diff_means(dat$x1,dat$Z)
obs_cov_bal

```

## What **should** we expect from an experiment?

So:

1. experiments do not guarantee exact equality of covariates and
2. we can **know** (or closely approximate) what kind of covariate differences
   a given experimental design would generate.



# Estimation, Estimators, Bias, Consistency, Given Randomization

## Estimating the ATE in an RCT:

Here are two proposals for estimating the ATE. How would we know whether either
or both of them work well (trick question)? (What do we want estimators to do for us?)

```{r echo=TRUE}
est1 <- function(Z,Y){
    mean(Y[Z==1]) - mean(Y[Z==0])
}
est2 <- function(Z,Y){
    coef(lm(Y~Z))[["Z"]]
}

with(dat,est1(Z=Z,Y=Y))
with(dat,est2(Z=Z,Y=Y))
```

## How does randomization help us trust our estimators?

This is a simulation assessing **estimation bias** (and hinting at **consistency**)

```{r, echo=TRUE}
## The truth:
with(dat,mean(y1-y0))

new_exp <- function(Z){
    ## This next shuffles Z
    newZ <- sample(Z)
    return(newZ)}

new_est <- function(newZ,y0,y1,the_est){
    newY <- newZ * y1 + (1-newZ)*y0
    result <- the_est(Z=newZ,Y=newY)
}

set.seed(1235)
dist_est1 <- with(dat, replicate(100, new_est(newZ=new_exp(Z),y0=y0,y1=y1,the_est=est1)))
mean(dist_est1)
```

## How does randomization help us trust our estimators?

Note: (1) Different simulations give slightly different results and (2) more simulations differ from each other less.


```{r, echo=TRUE}

set.seed(1235)
dist_est1a <- with(dat, replicate(100, new_est(newZ=new_exp(Z),y0=y0,y1=y1,the_est=est1)))
mean(dist_est1a)
dist_est1b <- with(dat, replicate(100, new_est(newZ=new_exp(Z),y0=y0,y1=y1,the_est=est1)))
mean(dist_est1b)

dist_est2a <- with(dat, replicate(10000, new_est(newZ=new_exp(Z),y0=y0,y1=y1,the_est=est1)))
mean(dist_est2a)
dist_est2b <- with(dat, replicate(10000, new_est(newZ=new_exp(Z),y0=y0,y1=y1,the_est=est1)))
mean(dist_est2b)
```

## How does randomization help us trust our estimators?

What did randomization provide here?

 1. Grounds for repetition (i.e. we **knew** how to repeat the generation of $Z$),
 2. No need to mention `x1` (we could check to see if we should worry about x1).
 3. an unbiased estimator.

## What about if we didn't know exactly how Z was assigned? {.allowframebreaks}

Imagine that `x1` causes `Z` (here, Z is randomized but x1 changes Z before revealing y0 or y1):

\begin{center}
\begin{tikzcd}[column sep=large]
	  Z  \arrow[from=1-1,to=1-4] &    &    & Y \\
	   x_1 \arrow[from=2-1,to=1-1] \arrow[from=2-1,to=1-4]
\end{tikzcd}
\end{center}


```{r biasexample, cache=TRUE, echo=TRUE}
new_biased_exp <- function(Z,x1){
    newZ1 <- sample(Z)
    # newZ <- newZ1*rbinom(10,size=1,prob=(x1+1)/4)
    newZ <- pnorm(x1+newZ1)>pnorm(median(x1+newZ1))
    return(as.numeric(newZ))
}
trueATE <- with(dat,mean(y1-y0))
with(dat,est1(new_biased_exp(Z,x1),Y))

set.seed(1235)
dist_est_biased <- with(dat, replicate(10000, new_est(newZ=new_biased_exp(Z,x1),y0=y0,y1=y1,the_est=est1)))
summary(trueATE-dist_est_biased)
## And recall our previous distribution of our estimator across randomizations
## This next is unbiased (mean \approx 0)
summary(trueATE-dist_est2a)
```

## What about if we didn't know exactly how Z was assigned?

Imagine that `x1` causes `Z` (here, Z is randomized but x1 changes Z before reveals y0 or y1):

```{r figbias, out.width=".7\\textwidth"}
plot(density(dist_est2a),ylim=c(0,.8))
lines(density(na.omit(dist_est_biased)),col="blue")
abline(v=with(dat,mean(y1-y0)))
points(c(mean(dist_est_biased,na.rm=TRUE),mean(dist_est2a)),c(0,0),pch=c(17,18),cex=2,col=c("blue","black"))
```

## What about if we didn't know how Z was assigned at all?

How would we assess bias if we didn't know that $x_1$ caused $Z$?

 - We cannot simply shuffle Z. Because we don't know how Z arose.
 - Do we know how x1 was generated? If so, we could re-generate x1 and *hope* that our x1 to Z function is right
 - We could repeatedly re-generate Y itself if we **knew** how it was created.
 - We could resample the entire dataset if we **knew** how it was sampled.

So: **known randomization allows us to assess bias.**

# Covariance Adjustment?

## What does linear regression do in an observational study?

Here is another bit of fake data where we know the true causal effects (the $\tau_i$ for each person and the $y_{i,1}, y_{i,0}$, too). In real life we'd only observe $Y$, $x_1, \ldots, x_4$, and $Z$.

```{r newdat, echo=FALSE}
N <- 100
tau <- .3
set.seed(12345)
dat <- data.frame(
  id = 1:N,
  x1 = rpois(n = N, lambda = 10),
  x2 = sample(1:6, size = N, replace = TRUE)
)

dat <- mutate(dat,
  y0 = .2 * x1 - .2 * x1^2 + .2 * (x2 < 2) + runif(n = N, min = -2 * sd(x1), max = 2 * sd(x1)),
  y0 = round(y0 + abs(min(y0)) / max(y0)),
  y0 = abs(ifelse(x1 < 3, 0, y0)),
  y1 = round(y0 + tau * sd(y0) + runif(n = N, min = -2 * tau * sd(y0), max = .5 * sd(y0))),
  x3 = rnorm(n(), mean = mean(x2), sd = sd(x2)),
  x4 = rbinom(n(), size = 1, prob = mean(x1 > 10))
)
## In an experiment we would control Z
## dat$Z <- complete_ra(N=N,m=floor(N/2))
dat$Z <- with(dat, as.numeric((.4 * sd(x1) * x1 + runif(n = N, min = -20, max = 0)) > 0))
## table(dat$Z)
## boxplot(x1~Z,data=dat)
## summary(lm(Z~x1,data=dat))$r.squared
dat <- mutate(dat, Y = Z * y1 + (1 - Z) * y0)
dat$tau <- with(dat, y1 - y0)
## summary(dat$tau)
kbl(head(dat[, c("id", "x1", "x2", "x3", "x4", "Z", "Y", "y1", "y0", "tau")]))
##  summary(lm(y0~x1,data=dat))$r.squared
##  blah <- lm_robust(Y~Z,data=dat); blah$p.value["Z"]
##  blah2 <- lm_robust(Y~Z+x1,data=dat); blah2$p.value["Z"]
##  with(dat,scatter.smooth(x1,Y,col=Z+1))
save(dat,file=here::here("day7_dat.rda"))
```

## What is the effect of Z on Y?

If we had a dataset, like, say, the number of miles people are willing to travel to get tested by COVID (`Y`) and whether they downloaded a COVID prevention information kit from a local US municipal government website, (`Z`), we could estimate the average causal effect of the COVID info kit like so:

```{r res1, echo=TRUE}
lm0 <- lm_robust(Y ~ Z, data = dat)
coef(lm0)
```

But how should we interpret this? It looks like the kit causes a reduction in
willingness to travel to be tested. This might be true. But we can immediately
think of **alternative explanations**:

 - Maybe people who download information kits differ from people who don't
   choose to download such kits in other ways --- they might be wealthier, more
   likely to have a computer (since looking at pdf brochures on an old phone is
   no fun), be more interested in reading about health, speak English
   (imagining that the kit is in English), etc..

\medskip

So, how might we try to set aside, or engage with, those alternative explanations?

## "Controlling for" to remove the effect of $x_1$ from $\hat{\bar{\tau}}$

A common approach looks like the following --- the "effect of $Z$ 'controlling for' $x_1$".

```{r lm1, echo=TRUE}
lm1 <- lm_robust(Y ~ Z + x1, data = dat)
coef(lm1)["Z"]
```

Recall that this is the problem --- a $Z \rightarrow Y$ relationship could easily just reflect the $x_1 \rightarrow Z$ and $x_1 \rightarrow Y$ relationships and not the $Z \rightarrow Y$ relationship.

\begin{center}
\begin{tikzcd}[column sep=large]
	  Z  \arrow[from=1-1,to=1-4] &    &                                                            & Y \\
	   x_1 \arrow[from=2-1,to=1-1] \arrow[from=2-1,to=1-4]
\end{tikzcd}
\end{center}


Today: Let's talk about what "controlling for" means. And then let's ask "How
would we know whether we did a good job --- did we "control for $x_1$"
**enough**?"

What does "controlling for" mean here? How can we explain it? 
<!-- Exercise for the class: Please explain what this means, as far as you can tell, here <https://www.menti.com/amwgborzsv>. -->

How would we know whether we did a good job --- did we "control for $x_1$" **enough**?


## First, recall how linear models control or adjust

Notice that the linear model **does not hold constant** $x_1$. Rather it
**removes a linear relationship** -- the coefficient of `r coef(lm1)[["Z"]]`
from `lm1` is **the effect of $Z$ after removing the linear relationship
between $x_1$ and $Y$ and between $x_1$ and $Z$**. (blue is treated)

```{r covadj2, echo=TRUE}
## Adjusting for only x1
lm1 <- lm_robust(Y ~ Z + x1, data = dat)
coef(lm1)["Z"]
### Notice that this is the same as what follows
lm_Y_x1 <- lm(Y ~ x1, data = dat)
lm_Z_x1 <- lm(Z ~ x1, data = dat)
dat$resid_Y_x1 <- resid(lm_Y_x1)
dat$resid_Z_x1 <- resid(lm_Z_x1)
lm1b<- lm(resid_Y_x1 ~ resid_Z_x1, data = dat)
coef(lm1b)[[2]]
stopifnot(all.equal(coef(lm1b)[[2]],coef(lm1)[["Z"]]))
```

## Linear models use residualization to remove linear relationships

What does **residualization do**? It removes linear relationships. Notice that the coefs on x1 below are 0.

```{r showresid, echo=TRUE}
lm_resid_Y_x1 <- lm(resid_Y_x1 ~ x1, data = dat)
lm_resid_Z_x1 <- lm(resid_Z_x1 ~ x1, data = dat)
coef(lm_resid_Y_x1)[["x1"]]
coef(lm_resid_Z_x1)[["x1"]]
```

## Linear models use residualization to remove linear relationships

Notice that this works with as many covariates as you'd like:

```{r bigresid, echo=TRUE}
lmYbig <- lm(Y~x1+x2+x3+x4,data=dat)
lmZbig <- lm(Z~x1+x2+x3+x4,data=dat)
coef(lm(resid(lmYbig)~resid(lmZbig)))[2]
lmbig<- lm(Y~Z+x1+x2+x3+x4,data=dat)
coef(lmbig)[["Z"]]
```

\medskip

`resid(lmYbig)` is $Y_i - \hat{Y}_i = Y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_{i,1} + \hat{\beta}_2 x_{i,2} + \hat{\beta}_3 x_{i,3} + \hat{\beta}_4 x_{i,4})$

\medskip

That is, `resid(lmYbig)` is just Y without its linear additive relationship with the covariates (the linear additive relationship that least squares would give you, not the one that, say a quantile regression would give you, or an outlier penalized regression or another target function).

## Removing x1 from Y and x1 from Z and then describing the Z $\rightarrow$ Y linear relationship

Notice that the linear model **does not hold constant** $x_1$. Rather it
**removes a linear relationship** -- the coefficient of `r coef(lm1)[["Z"]]`
from `lm1` is **the effect of $Z$ after removing the linear relationship
between $x_1$ and $Y$ and between $x_1$ and $Z$**. (blue is treated)

```{r plotresids, echo=FALSE,out.width=".7\\textwidth"}
par(mfrow = c(2, 2), mar = c(3, 3, 1, 0), mgp = c(1.25, .5, 0), oma = rep(0, 4))
with(dat, plot(x1, Y, col = c("black", "blue")[dat$Z + 1], pch = c(1, 19)[dat$Z + 1]))
abline(lm_Y_x1)
with(dat, plot(x1, resid_Y_x1, ylab = "Y - b*x1 or Y without linear relation with x1", col = c("black", "blue")[dat$Z + 1], pch = c(1, 19)[dat$Z + 1]))
abline(lm_resid_Y_x1)
# with(dat,plot(x1,jitter(Z,factor=.1)))
with(dat, plot(x1, Z, col = c("black", "blue")[dat$Z + 1], pch = c(1, 19)[dat$Z + 1]))
abline(lm_Z_x1)
with(dat, plot(x1, resid_Z_x1, ylab = "Z - b*x1 or Z without linear relation with x1", col = c("black", "blue")[dat$Z + 1], pch = c(1, 19)[dat$Z + 1]))
abline(lm_resid_Z_x1)
```

## Recall how linear models control or adjust

Notice that the linear model **does not hold constant** $x_1$. Rather it **removes a linear relationship** -- the coefficient of `r coef(lm1)[["Z"]]` from `lm1` is **the effect of $Z$ after removing the linear relationship between $x_1$ and $Y$ and between $x_1$ and $Z$**. (blue=treated, black=control).

```{r echo=FALSE,out.width=".7\\textwidth"}
with(dat, plot(resid_Z_x1, resid_Y_x1, , col = c("black", "blue")[dat$Z + 1], pch = c(1, 19)[dat$Z + 1]))
abline(lm1b)
```

##  Recall how linear models control or adjust

How might this plot help us make decisions about the adequacy of our linear model adjustment strategy? Signs of extrapolation? Non-linearity? (Notice all the blue points with no open circles underneath them --- in that range of the plot, our "adjustment" results entirely from the assumption of linearity. We don't know what would happen if we were to observe the open circles: would they follow a straight line?)

```{r plot2, out.width=".7\\textwidth"}
par(mfrow = c(1, 1))
dat$ZF <- factor(dat$Z)
with(dat, plot(x1, jitter(Y), col = c("black", "blue")[dat$Z + 1], pch = c(1, 19)[dat$Z + 1]))
preddat <- expand.grid(Z = c(0, 1), x1 = sort(unique(dat$x1)))
preddat$fit <- predict(lm1, newdata = preddat)
with(preddat[preddat$Z == 0, ], lines(x1, fit))
with(preddat[preddat$Z == 1, ], lines(x1, fit, col = "blue", lwd = 2))
```

##  What about improving the model?

Does this help? (not really. Why squared? Why not cubed? Why not cut into pieces? Why not...?)

```{r echo=TRUE}
lm2 <- lm(Y ~ Z + x1 + I(x1^2), data = dat)
coef(lm2)[["Z"]]
```

```{r lm1andlm2, echo=FALSE,out.width=".6\\textwidth"}
par(mfrow = c(1, 1))
dat$ZF <- factor(dat$Z)
with(dat, plot(x1, jitter(Y), col = c("black", "blue")[dat$Z + 1], pch = c(1, 19)[dat$Z + 1]))
with(preddat[preddat$Z == 0, ], lines(x1, fit))
with(preddat[preddat$Z == 1, ], lines(x1, fit, col = "blue", lwd = 2))
preddat$fit2 <- predict(lm2, newdata = preddat)
with(preddat[preddat$Z == 0, ], lines(x1, fit2))
with(preddat[preddat$Z == 1, ], lines(x1, fit2, col = "blue", lwd = 2))
```

##  What about when we control for more than one variable? {.shrink}


Is this better? Or worse? (It depends on whether we want to remove additive and linear relationships.)

```{r lm3, echo=TRUE}
lm3 <- lm(Y ~ Z + x1 + x2 + x3 + x4, data = dat)
coef(lm3)[["Z"]]
```

We could still residualize (removing the multidimensional linear relationship):


```{r lm3res, echo=TRUE}
dat$resid_Y_xs <- resid(lm(Y ~ x1 + x2 + x3 + x4, data = dat))
dat$resid_Z_xs <- resid(lm(Z ~ x1 + x2 + x3 + x4, data = dat))
lm3_resid <- lm(resid_Y_xs ~ resid_Z_xs, data = dat)
coef(lm3_resid)[[2]]
```
##  What about when we control for more than one variable?

Is this better? Or worse? Hard to tell. What should we be looking for?

```{r plotres2, echo=TRUE}
with(dat, plot(resid_Z_xs, resid_Y_xs, col = c("black", "blue")[dat$Z + 1], pch = c(1, 19)[dat$Z + 1]))
abline(lm3_resid)
```

##  What about when we control for more than one variable? {.allowframebreaks}

Does adding variables help? (Here we can see influential points using the
Cook's D statistic. See the code for the different specifications.) Notice that
as we add variables, or make the "controlling for" part more complicated, the
more single points start to exert more and more influence over the results.

```{r plotcooks, eval=FALSE, echo=FALSE, results=FALSE, outwidth=".8\\textwidth"}
par(mfrow = c(2, 2), pty = "m", mgp = c(1.25, .5, 0), mar = c(3, 3, 2, 0), oma = c(0, 0, 0, 0))
plot(lm3, which = c(1, 3, 5, 6), col = c("black", "blue")[dat$Z + 1], , pch = c(1, 19)[dat$Z + 1])
```

```{r cooksplots, echo=TRUE, warning=FALSE}
library(olsrr)
library(splines)
library(gridExtra)
v1 <- ols_plot_cooksd_bar(lm(Y ~ Z + x1, data = dat), print_plot = FALSE)
v2 <- ols_plot_cooksd_bar(lm(Y ~ Z + x1 + x2, data = dat), print_plot = FALSE)
v3 <- ols_plot_cooksd_bar(lm(Y ~ Z + x1 + x2 + x3, data = dat), print_plot = FALSE)
v4 <- ols_plot_cooksd_bar(lm(Y ~ Z + x1 + x2 + x3 + x4, data = dat), print_plot = FALSE)
v5 <- ols_plot_cooksd_bar(lm(Y ~ Z + poly(x1, 3) + poly(x2, 2) + poly(x3, 4) + x4, data = dat), print_plot = FALSE)
v6 <- ols_plot_cooksd_bar(lm(Y ~ Z + I(cut(x1, 3)) * I(cut(x2, 3)) * I(cut(x3, 3)) * x4, data = dat), print_plot = FALSE)
v7 <- ols_plot_cooksd_bar(lm(Y ~ Z * x1 * x2 * x3 * x4, data = dat), print_plot = FALSE)
v8 <- ols_plot_cooksd_bar(lm(Y ~ Z + ns(x1, 3) + ns(x2, 3) * ns(x3, 3) * x4, data = dat), print_plot = FALSE)

plots <- lapply(1:8, function(i) {
  newplot <- get(paste0("v", i))$plot
  return(newplot + ggtitle(paste0("v", i)) + theme(legend.position = "none"))
})

cooksd_plot <- marrangeGrob(plots, nrow = 2, ncol = 4)
ggsave("cooksd.pdf", cooksd_plot, width = 12, height = 6)
```

\includegraphics[width=.9\linewidth]{cooksd.pdf}

## How to choose? Maybe a specification curve? .{allowframebreaks}

How many choices do we have? Should we try as many choices as possible?^[see <https://masurp.github.io/specr/index.html> for more citations]

```{r specr, echo=TRUE}
library(specr)
## see https://cran.r-project.org/web/packages/specr/vignettes/getting-started.html
## possible covariates:
library(splines)
basecovs <- c("x1", "x2", "x3", "x4")
mf <- model.frame(Y ~ Z + x1 * x2 * x3 * x4 + x1 * poly(x1, 3) + x2 * poly(x2, 2) + x3 * poly(x3, 4) + ns(x1, 3) + ns(x2, 3) + ns(x3, 3) +
  I(cut(x1, 3)) * I(cut(x2, 3)) * I(cut(x3, 3)), data = dat)
mm <- model.matrix(mf, data = dat)
thedat <- data.frame(mm[, -1])
thedat$Y <- dat$Y

specr_setup_obj <- specr::setup(
  data = thedat,
  y = c("Y"),
  x = c("Z"),
  model = c("lm"),
  # controls = grep("^x|^poly|^I|^ns",names(thedat),value=TRUE))
  controls = c(
    "x1", "x2", "x3", "x4",
    "poly(x1,3)",
    "poly(x1,2)",
    "poly(x2,2)",
    "poly(x3,2)",
    "poly(x3,3)",
    "poly(x3,4)"
  )
)
```

```{r echo=TRUE}
summary(specr_setup_obj)
results <- specr(specr_setup_obj)
summary(results)
summary(results$data$estimate)
```

## How to choose? A specification curve.

How many choices do we have? Should we try as many choices as possible?^[see <https://masurp.github.io/specr/index.html> for more citations]

```{r plotspecs, out.width=".9\\textwidth",eval=TRUE}
plot(results, choices = c("controls"), ci = FALSE, rel_heights = c(1, 4), ribbon = TRUE)
# plot_curve(results)
```


## How to choose? Choosing different break-points. {.shrink}

How many choices do we have? Should we try as many choices as possible?

```{r exploremanycuts, echo=TRUE, results="markup", cache=TRUE}
lmadjfn <- function() {
  covs <- c("x1", "x2", "x3", "x4")
  ncovs <- sample(1:length(covs), 1)
  somecovs <- sample(covs, size = ncovs)
  ncuts <- round(runif(ncovs, min = 1, max = 8))
  theterms <- ifelse(ncuts == 1, somecovs,
    paste("cut(", somecovs, ",", ncuts, ")", sep = "")
  )
  thefmla <- reformulate(c("Z", theterms), response = "Y")
  thelm <- lm(thefmla, data = dat)
  theate <- coef(thelm)[["Z"]]
  return(theate)
}

set.seed(12345)
res <- replicate(10000, lmadjfn())
summary(res)
```

## How to choose? Choosing different break-points. {.shrink}

How many choices do we have? Should we try as many choices as possible? Here
are the estimates of $Z \rightarrow Y$ from 10,000 different ways to "control
for" $x_1,x_2,x_3,x_4$.

```{r plotres}
plot(density(res))
rug(res)
```

Ack. Which one should we choose? It seems like adjusting for covariates using
linear models raises many more questions than it answers, and it is not clear
how to answer all of those questions!

## How about stratification?

Ok. So at this point we are not going to use linear models to adjust for
covariates. What to do? What about simplifying the problem? When a person wants
to know whether we have "controlled for", say, $x_4$, I suspect they are really
asking for this:

```{r strat1, echo=TRUE}
lm_x4_0 <- lm(Y ~ Z, data = dat, subset = x4 == 0)
lm_x4_1 <- lm(Y ~ Z, data = dat, subset = x4 == 1)
coef(lm_x4_1)[["Z"]]
coef(lm_x4_0)[["Z"]]
```

In this case we can say that we have "held constant" $x_4$. But what is the **overall estimate** in this case?

\medskip

Choosing an additive and linear functional form allows us to predict $Y$ for any given $x$ where the differences in predicted Y relate to differences in x in a constant way with respect to the other variables. But this an implication or consequence of the linearity and additivity choice.




## Estimate an overall ATE with stratification {.allowframebreaks}

We know how to analyze a block-randomized (or strata-randomized) experiment
(see [@gerbergreen2012]): each block is a mini-experiment. We *estimate the ATE
within each block* and *combine by weighting each block specific estimate*.

The block-size weight produces an unbiased estimator in randomized experiments
--- in an observational study we don't know about the bias since we don't
exactly know how to repeat the study.  The precision weight (aka the "fixed
effects" weights) tends to produce smaller standard errors and confidence
intervals but is biased in randomized experiments.

```{r weighting, echo=TRUE}
dat_sets <- dat %>%
  group_by(x4) %>%
  summarize(
    nb = n(),
    ateb = mean(Y[Z == 1]) - mean(Y[Z == 0]),
    prob_trt = mean(Z),
    nbwt = n() / nrow(dat),
    prec_wt = nbwt * prob_trt * (1 - prob_trt),
  )

dat_sets$prec_wt_norm <- with(dat_sets, prec_wt / sum(prec_wt))

print(dat_sets)

est_ate1 <- with(dat_sets, sum(ateb * nbwt))
est_ate2 <- with(dat_sets, sum(ateb * prec_wt / (sum(prec_wt))))
```

## Estimate an overall ATE with stratification? {.allowframebreaks}

Block-  or strata-level weights can also be represented at the individual level --- and this allows us to use linear models (least squares) to produce block-weighted estimates of the overall average causal effect after "holding constant" $x_4$.

```{r echo=TRUE}
## Now at the individual level
dat <- dat %>%
  group_by(x4) %>%
  mutate(
    nb = n(),
    mb = sum(Z),
    ateb = mean(Y[Z == 1]) - mean(Y[Z == 0]),
    prob_trt = mean(Z),
    nbwt = (Z / prob_trt) + (1 - Z) / (1 - prob_trt),
    prec_wt = nbwt * prob_trt * (1 - prob_trt)
  ) %>%
  ungroup()

## Two ways to use the block-size weight
est_ate1a <- difference_in_means(Y ~ Z, blocks = x4, data = dat)
est_ate1b <- lm_robust(Y ~ Z, weights = nbwt, data = dat)
est_ate1c <- lm(Y ~ Z, weights = nbwt, data = dat)

## Three other ways to use the precision or harmonic weight
est_ate2a <- lm_robust(Y ~ Z + x4, data = dat)
est_ate2b <- lm_robust(Y ~ Z, fixed_effects = ~x4, data = dat)
est_ate2c <- lm_robust(Y ~ Z, weights = prec_wt, data = dat)

c(est_ate1, coef(est_ate1a)[["Z"]], coef(est_ate1b)[["Z"]], coef(est_ate1c)[["Z"]])
c(est_ate2, coef(est_ate2a)[["Z"]], coef(est_ate2b)[["Z"]], coef(est_ate2c)[["Z"]])
```

## Finally, what about variable selection? {.allowframebreaks}

We could use a penalized model (like the lasso or adaptive lasso) or some other approach (like random forests) to **automatically choose** a specification.

```{r glmnet1, echo=TRUE}
## Here using the mm data with polynomials
library(glmnet)
cv1 <- cv.glmnet(mm[, 3:15], y = dat$Y)
```

```{r cvplot, out.width=".6\\textwidth"}
coefs_lasso <- coef(cv1$glmnet.fit)
coefnms <- row.names(coefs_lasso)
plot(cv1$glmnet.fit, xvar = "lambda")
abline(v = .743)
text(x = rep(-4, 13), y = coefs_lasso[-1, 65], labels = coefnms)
```

```{r}
sol1 <- coef(cv1$glmnet.fit, s = .743)
sol1
```

 - Of course, then we have to argue that our **tuning parameter choice** made sense.
 - And, again, we have no standard for knowing when we have done **enough**.

# Summary and Overview

## Benefits of Randomized Designs

Randomization makes $y_1, y_0, \bX \perp Z$. How to make use of this fact in a randomized experiment?

 1. Interpretable comparisons (lack of omitted variable bias, confounding,
    selection bias)
      - Can I interpret differences in outcome as caused by $Z$ and not $X$?
	Is it easy to confuse the effect of $Z$  with the  effects of $X$?
      - How does  randomization do this? How  does randomization eliminate
	**alternative explanations**? (Recall that it does not exactly balance $X$.)
 2. Reliable statistical inferences (estimators and tests)
      - The idea of **design-based** versus **model-based** statistical
	inference (next few slides).

## Design Based Approach: Estimate Averages

  1. Notice that the observed $Y_i$ are a sample from  the (small, finite)
     population of $(y_{i,1},y_{i,0})$.
  2. Decide to focus on the average, $\bar{\tau}$, because sample averages,
     $\hat{\bar{\tau}}$ are unbiased and consistent estimators of population
     averages under random sampling (where no covariate determines the sample
     inclusion probability or assignment to $Z_i$).
  3. Estimate $\bar{\tau}$ with the observed difference in means.

\centering
  \includegraphics[width=.5\textwidth]{images/cartoonNeyman.pdf}

## Design Based Approach: Estimate Averages

\centering
  \includegraphics[width=.9\textwidth]{images/cartoonNeyman.pdf}

## What about when we have not randomized?

We can try to adjust (for example by "controlling for". But adjustment raises new questions: how to adjust? how to assess our adjustment?) After all **Regression is not research design**

Ideas for a simpler way to say "We have held x1 constant?" as a way to discard alternative explanations based on x1?

## Lingering Questions?

# Appendix

## Design Based Approach: Test Hypotheses

 1. Make a guess about $\tau_i$.
 2. Then measure surprise or consistency of data with this guess given the
    design. (Given all of the ways this experiment could have  occurred, how
    many look more extreme than what we observe? Does our observation look
    typical or rare?)

\centering
  \includegraphics[width=.6\textwidth]{images/cartoonFisherNew1.pdf}

## Design Based Approach: Test Hypotheses

\centering
  \includegraphics[width=\textwidth]{images/cartoonFisherNew1.pdf}

## Design Based Approach: Test Hypotheses

\centering
  \includegraphics[width=\textwidth]{images/cartoonFisherNew2.pdf}

## Approaches to creating interpretable comparisons:

   - Randomized experiments (more precision from reducing heterogeneity in $Y$)
   - Instrumental variables (with randomized $Z$ created $D$)
   - Natural Experiments / Discontinuities (one $X$ creates $Z$) (includes RDD)
   - Difference-in-Differences (reduce bias *and* increase precision from reducing heterogeneity in $Y$)
   - Semi-/Non-parametric Covariance Adjustment (ex. Matching)
   - Parametric covariance adjustment

## References

