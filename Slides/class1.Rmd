---
title: |
  | Causal Inference for Observational Studies
  | Beyond the Basics
  | Class 1
date: '`r format(Sys.Date(), "%B %d, %Y")`'
author: Jake Bowers
bibliography:
 - ../BIB/references.bib
fontsize: 10pt
geometry: margin=1in
graphics: yes
biblio-style: authoryear-comp
biblatexoptions:
  - natbib=true
output:
  beamer_presentation:
    slide_level: 2
    keep_tex: true
    latex_engine: xelatex
    citation_package: biblatex
    template: icpsr.beamer
    includes:
        in_header:
           - defs-all.sty
    md_extensions: +raw_attribute
header-includes:
  - \setbeameroption{hide notes}
---

<!-- To show notes  -->
<!-- https://stackoverflow.com/questions/44906264/add-speaker-notes-to-beamer-presentations-using-rmarkdown -->

```{r echo=FALSE, include=FALSE, cache=FALSE}
# Some customization.  You can alter or delete as desired (if you know what you are doing).
# knitr settings to control how R chunks work.
rm(list=ls())

require(knitr)

## This plus size="\\scriptsize" from https://stackoverflow.com/questions/26372138/beamer-presentation-rstudio-change-font-size-for-chunk
knitr::knit_hooks$set(mysize = function(before, options, envir) {
			      if (before)
				      return(options$size)
})

knit_hooks$set(plotdefault = function(before, options, envir) {
		       if (before) par(mar = c(3, 3, .1, .1),oma=rep(0,4),mgp=c(1.5,.5,0))
})

opts_chunk$set(
	       tidy=TRUE,
	       echo=TRUE,
	       results='markup',
	       strip.white=TRUE,
	       fig.path='figs/fig',
	       cache=FALSE,
	       highlight=TRUE,
	       width.cutoff=132,
	       size='\\scriptsize',
	       out.width='.7\\textwidth',
	       fig.retina=FALSE,
	       message=FALSE,
	       comment=NA,
	       mysize=TRUE,
	       plotdefault=TRUE)

options(digits=4,
	scipen=8,
	width=132,
	show.signif.stars=FALSE)
```

```{r loadlibs,include=FALSE, echo=FALSE}
library(dplyr)
library(ggplot2)
library(sandwich)
library(lmtest)
library(estimatr)
```

```{r makedirs, echo=FALSE, include=FALSE}
if(!dir.exists('figs')) dir.create('figs')

## Make a local library directory
if(!dir.exists(here::here('libraries'))){
        dir.create(here::here('libraries'))
}
```


```{r eval=TRUE, include=FALSE, echo=FALSE}
library('devtools')
library('withr')

with_libpaths(here::here('libraries'), install_github("markmfredrickson/RItools"), 'pre')
library('RItools',lib.loc = here::here('libraries'))
library(optmatch)
```

# Overview and Review

## Doing Matching, Assessing Designs, Estimating Effects

  1. **Review** Causal inference in randomized experiments; Statistical
     inference for causal effects in randomized experiments \autocite[Chap
     2]{rosenbaum2010}, \autocite[Chap 1-3]{gerbergreen2012}.

  2. **How to use optimal, full matching to produce a matched research
     design?**  Multivariate optimal matching review using `optmatch` in R:
     producing matched research designs using matching on scalars, propensity
     scores, and Mahalanobis distances \autocite{hansen2004}, \autocite[Chap
     1,3,7,8,9,13]{rosenbaum2010}, \autocite[Chap 9.0--9.2]{gelman2007dau}.

  3. **How to reason about whether we have a good matched research design?**
     Multivariate balance assessment using null hypothesis testing using the
     `RItools` package for R \autocite{hansenbowers2008} (and perhaps also
     using equivalence testing \autocite{hartman2018equivalence}).

  4. **How to customize and focus the matched research design creation?**
     Multivariate optimal matching; calipers; penalties; combining scores

  5. **How to estimate the ATE  and test hypotheses about causal effects given a
     matched research design?** Estimating ATE and Std Errors from Matched
     Designs using the Block Randomized Experiment as an Analogy.

## Overly Ambitious Plan

  - 09:00--09:30 ---  Introductions: Name, Affiliation/Organization, Interest (Substantive or Methodological)
  - 09:30--10:15 ---  Lecture by Jake to introduce concepts and encourage questions and answers.
  - 10:15--10:45 ---  Exercises by Class to raise even more questions.
  - 10:45 -- 11:00 --- Break
  - 11:00 -- 11:30 --- Lecture by Jake / question and answers about the previous lecture
  - 11:30 -- 12:00 --- Open Discussion on any topic.

## What do you know about causal inference?

These words are used a lot in discussions of causal inference. Why are they important? What do they mean?

  - Potential Outcomes; Counterfactual Outcomes
  - Causal Effect
  - ATE
  - Randomization
  - Balance
  - Propensity Score
  - Mahalanobis Distance Score
  - Confounding; Selection Bias; Omitted Variable Bias
  - Covariates

What other words and/or phrases and/or concepts come to mind when you think about causal inference?

\medskip

Why is "randomization" listed here if a lot of the discussion about causal inference focuses on observational studies?

## Randomization and Causal Inference

From last week:

  - *Treatment* $Z_i=1$ for treatment and $Z_i=0$ for control for units $i$

  - Each unit has a pair of *potential outcomes* $(y_{i,Z_i=1},y_{i,Z_i=0})$
    (also written  $(y_{i,1},y_{i,0})$ ) (given SUTVA).

  - *Causal Effect*  when $y_{i,1} \ne y_{i,0}$, $\tau_i   =
    f(y_{i,1},y_{i,0})$ ex. $\tau_i =  y_{i,1} - y_{i,0}$.

  - *Fundamental Problem of (Counterfactual) Causality* We only see one
    potential outcome $Y_i = Z_i * y_{i,1} + (1-Z_i) y_{i,0}$

  - *Covariates*,  $\bX=\begin{bmatrix} x_{11} & \ldots & x_{1k} \\ \vdots &
    \vdots & \vdots \\  x_{n1} & \ldots & x_{nk} \end{bmatrix}$ is a matrix
    containing  background information about the units that might predict
    $(y_{i,1},y_{i,0})$ or $Z$ (except in an experiment).

Randomization makes $y_1, y_0, \bX \perp Z$. How to make use of this fact in a randomized experiment?

## Benefits of Randomized Designs

 1. Interpretable comparisons (lack of omitted variable bias, confounding,
    selection bias)
      - Can I interpret differences in outcome as caused by $Z$ and not $X$?
	Is it easy to confuse the effect of $Z$  with the  effects of $X$?
      - How does  randomization do this? How  does randomization eliminate
	**alternative explanations**?
 2. Reliable statistical inferences (estimators and tests)
      - The idea of **design-based** versus **model-based** statistical
	inference (next few slides).


## Design Based Approach 1: Test Hypotheses

 1. Make a guess about $\tau_i$.
 2. Then measure surprise or consistency of data with this guess given the
    design. (Given all of the ways this experiment could have  occurred, how
    many look more extreme than what we observe? Does our observation look
    typical or rare?)

\centering
  \includegraphics[width=.6\textwidth]{images/cartoonFisherNew1.pdf}

## Design Based Approach 1: Test Hypotheses

\centering
  \includegraphics[width=\textwidth]{images/cartoonFisherNew1.pdf}

## Design Based Approach 1: Test Hypotheses

\centering
  \includegraphics[width=\textwidth]{images/cartoonFisherNew2.pdf}



## Design Based Approach 2: Estimate Averages

  1. Notice that the observed $Y_i$ are a sample from  the (small, finite) population of $(y_{i,1},y_{i,0})$.
  2. Decide to focus on the average, $\bar{\tau}$, because sample averages, $\hat{\bar{\tau}}$ are unbiased and consistent estimators of population averages.
  3. Estimate $\bar{\tau}$ with the observed difference in means.

\centering
  \includegraphics[width=.5\textwidth]{images/cartoonNeyman.pdf}

## Design Based Approach 2: Estimate Averages

\centering
  \includegraphics[width=.9\textwidth]{images/cartoonNeyman.pdf}


## Approaches to creating interpretable comparisons:

   - Randomized experiments (more precision from reducing heterogeneity in $Y$)
   - Instrumental variables (with randomized $Z$ created $D$)
   - Natural Experiments / Discontinuities (one $X$ creates $Z$) (includes RDD)
   - Difference-in-Differences (reduce bias *and* increase precision from reducing heterogeneity in $Y$)
   - Semi-/Non-parametric Covariance Adjustment (ex. Matching)
   - Parametric covariance adjustment

## Lingering Questions?


# Doing Multivariate Matching

##  Did Transportation Reduce Crime?

```{r loaddata, echo=FALSE, cache=TRUE}
load(url("http://jakebowers.org/Data/meddat.rda"))
```

Cerdá et al. collected data on about roughly `r nrow(meddat)`
neighborhoods in Medellin, Colombia. About  `r signif(sum(meddat$nhTrt),2)` of had
access to the new Metrocable line and `r signif(sum(1-meddat$nhTrt),2)` did not.

\centering
\includegraphics[width=.7\textwidth]{medellin-gondola.jpg}

See also <https://www.medellincolombia.co/where-to-stay-in-medellin/medellin-orientation/> and <https://archleague.org/article/connective-spaces-and-social-capital-in-medellin-by-jeff-geisinger/>

##  Did Transportation Reduce Crime?

Cerdá et al. collected data on about roughly `r nrow(meddat)`
neighborhoods in Medellin, Colombia. About  `r signif(sum(meddat$nhTrt),2)` of had
access to the new Metrocable line and `r signif(sum(1-meddat$nhTrt),2)` did not.

\centering
\includegraphics[width=.8\textwidth]{medellin-conc-pov.jpg}


## Codebook/Covariates

We don't have a formal codebook. Here are some guesses about the meanings of
some of the variables. There are more variables in the data file than those
listed here.

\scriptsize
```
## The Intervention
nhTrt        Intervention neighborhood (0=no Metrocable station, 1=Metrocable station)

## Some Covariates (there are others, see the paper itself)
nh03         Neighborhood id
nhGroup      Treatment (T) or Control (C)
nhTrt        Treatment (1) or Control (0)
nhHom        Mean homicide rate per 100,000 population in 2003
nhDistCenter Distance to city center (km)
nhLogHom     Log Homicide (i.e. log(nhHom))

## Outcomes (BE03,CE03,PV03,QP03,TP03 are baseline versions)
BE      Neighborhood amenities Score 2008
CE      Collective Efficacy Score 2008
PV      Perceived Violence Score 2008
QP      Trust in local agencies Score 2008
TP      Reliance on police Score 2008
hom     Homicide rate per 100,000 population Score 2008-2003 (in log odds)

HomCount2003 Number of homicides in 2003
Pop2003      Population in 2003
HomCount2008 Number of homicides in 2008
Pop2008      Population in 2008
```

## How did the Metrocable effect Homicides?


```{r ratesfromcounts, echo=FALSE}
meddat<- mutate(meddat, HomRate03=(HomCount2003/Pop2003)*1000,
                HomRate08=(HomCount2008/Pop2008)*1000)
```

One approach:  Estimate the average treatment effect of Metrocable on
Homicides after the stations were built.

```{r threeestates}
themeans<-group_by(meddat,nhTrt) %>% summarise(ybar=mean(HomRate08))
themeans
diff(themeans$ybar)
lmOne <- lm(HomRate08~nhTrt,meddat)
coef(lmOne)["nhTrt"]
library(estimatr)
difference_in_means(HomRate08~nhTrt,meddat)
```

\bigskip

**But** alternative explanations?

## What are alternative explanations for this effect?

We claim that the policy intervention had some effect. What are alternative explanations?

## Do we have any concerns about confounding?

Sometimes people ask about "bias from observed confounding" or "bias from selection on observables".
\medskip

The `xBalance` function tests the hypothesis that treatment assignment is independent of covariates.

```{r}
## xBalance(treatment~covariate)
xbMed1 <- xBalance(nhTrt~nhAboveHS,data=meddat,report="all")
xbMed1$overall ##  the overall chi-sq test
xbMed1$results  ## the covariate specific z-test
```

## How would you adjust for Proportion Above HS Degree?

Part of the Metrocable effect is not about Metrocable per se, but rather about
the education of people in the neighborhood. How should we remove `nhAboveHS`
from our estimate or test? What strategies can you think of?


## One approach to this problem: model-based adjustment

Let's try to just adjust for this covariate in a very common manner:

```{r echo=FALSE}
lm1 <- lm(HomRate08~nhTrt+nhAboveHS,data=meddat)
```

```{r echo=FALSE}
preddat <- expand.grid(nhTrt=c(0,1),nhAboveHS=range(meddat$nhAboveHS))
preddat$fit <- predict(lm1,newdata=preddat)
```

\centering
```{r, out.width=".9\\textwidth", echo=FALSE}
par(oma=rep(0,4),mgp=c(1.5,.5,0),mar=c(3,3,0,0))
with(meddat, plot(nhAboveHS,HomRate08,pch=c(1,2)[nhTrt+1]))
with(subset(preddat,subset=nhTrt==0),lines(nhAboveHS,fit,lty=1))
with(subset(preddat,subset=nhTrt==1),lines(nhAboveHS,fit,lty=2))
## locator()
text(c(0.111807,0.001629), c(1.871,2.204), labels=c("Treat","Control"),pos=1)
text(c(.3,.5),c( coef(lm1)[1]+coef(lm1)[3]*.3 , coef(lm1)[1]+coef(lm1)[2]+coef(lm1)[3]*.5),
     labels=c("Control","Treat"))
```

## Exactly what does this kind of adjustment do?

Notice that I can get the same coefficient (the effect of Metrocable on
Homicides adjusted for HS-Education in the neighborhood) either directly (as
earlier) or via **residualization**:

```{r}
coef(lm1)["nhTrt"]
eYX <- residuals(lm(HomRate08~nhAboveHS,data=meddat))
eZX <- residuals(lm(nhTrt ~ nhAboveHS, data=meddat))
lm1a <- lm(eYX~eZX)
coef(lm1a)[2]
```

## Exactly what does this kind of adjustment do?

So, how would you explain what it means to "control for HS-Education" here? 

```{r}
plot(eZX,eYX)
```



## Did we adjust enough? 

Maybe adding some more information to the plot can help us decide whether, and to what extend, we effectively "controlled for" the proportion of the neighborhood with more than High School education. Specifically, we might be interested in assessing extrapolation/interpolation problems arising from our linear assumptions.

\centering
```{r, out.width=".7\\textwidth", echo=FALSE, warning=FALSE, message=FALSE}
par(oma=rep(0,4),mgp=c(1.5,.5,0),mar=c(3,3,0,0))
with(meddat, plot(nhAboveHS,HomRate08,pch=c(1,2)[nhTrt+1]))
with(subset(preddat,subset=nhTrt==0), lines(nhAboveHS,fit,lty=1))
with(subset(preddat,subset=nhTrt==1),lines(nhAboveHS,fit,lty=2))
with(subset(meddat,subset=nhTrt==0),lines(loess.smooth(nhAboveHS,HomRate08,deg=1,span=2/3),lty=1))
with(subset(meddat,subset=nhTrt==1),lines(loess.smooth(nhAboveHS,HomRate08,deg=1,span=2/3),lty=2))
## locator()
text(c(0.111807,0.001629), c(1.871,2.204), labels=c("Treat","Control"),pos=1)
text(c(.3,.5),c( coef(lm1)[1]+coef(lm1)[3]*.3 , coef(lm1)[1]+coef(lm1)[2]+coef(lm1)[3]*.5),
     labels=c("Control","Treat"))
with(subset(meddat,subset=nhTrt==0),rug(nhAboveHS))
with(subset(meddat,subset=nhTrt==1),rug(nhAboveHS,line=-.5))
```

How should we interpret this adjustment? How should we judge the improvement that we made? What concerns might we have?



```{r echo=FALSE, eval=FALSE}
thecovs <- unique(c(names(meddat)[c(5:7,9:24)],"HomRate03"))
balfmla<-reformulate(thecovs,response="nhTrt")

xbMed<-balanceTest(balfmla,
	      data=meddat,
	      report=c("all"),
        p.adjust.method="none")
xbMed$overall
xbMed$results["nhAboveHS",,]
```


```{r echo=FALSE, eval=FALSE}
outcomefmla <- reformulate(c("nhTrt",thecovs),response="HomRate08")
lmbig <- lm(outcomefmla,data=meddat)
```

## Back to the randomized experiment to help make a case for appropriate adjustment

It seems like arguments about plots may be difficult to resolve. How about going back to `xBalance`? The idea of a standard against which we can compare a given design. How might we do this in this case?


*Attempt 1: Stratification ... but two estimates rather than one adjusted
estimate*

```{r}
lm1a <- lm(HomRate08~nhTrt,data=meddat,subset=nhAboveHS>=.1)
lm1b <- lm(HomRate08~nhTrt,data=meddat,subset=nhAboveHS<=.1)
```

*Attempt 2: Stratified adjustment*

```{r}
lm1c <- lm(HomRate08~nhTrt+I(nhAboveHS>=.1),data=meddat)
coef(lm1c)["nhTrt"]
```

And balance assessment after stratification:

```{r}
xbHS1 <- xBalance(nhTrt~nhAboveHS,strata=list(hs=~I(nhAboveHS>=.1)),data=meddat,report="all")
xbHS1$overall
xbHS1$results
```

## The Curse of Dimensionality and linear adjustment for one more variable.

What about more than one variable? Have we controlled for both population
density and educational attainment enough? How would we know? 

```{r}
lm2x <- lm(HomRate08 ~ nhTrt + nhPopD + nhAboveHS, data=meddat)
coef(lm2x)["nhTrt"]
```

Maybe another plot?

```{r eval=FALSE}
meddat$nhTrtF <- factor(meddat$nhTrt)
library(car)
scatter3d(HomRate08~nhAboveHS+nhPopD,
	  groups=meddat$nhTrtF,
	  data=meddat,surface=TRUE,
    fit=c("linear")) #additive"))
```

```{r echo=FALSE, eval=FALSE}
scatter3d(HomRate08~nhAboveHS+nhPopD,
	  groups=meddat$nhTrtF,
	  data=meddat,surface=TRUE,
    fit=c("additive"))

```


## Can we improve stratified adjustment?

Rather than two strata, why not three?

```{r}
lm1cut3 <- lm(HomRate08~nhTrt+cut(nhAboveHS,3),data=meddat)
coef(lm1cut3)["nhTrt"]
```
But why those cuts? And why not 4? Why not...?

\medskip

One idea: collect observations into strata such that the sum of the
differences in means of nhAboveHS within strata is smallest? This is the idea
behind `optmatch` and other matching approaches.

## The optmatch workflow: The distance matrix

Introduction to `optmatch` workflow. To minimize differences requires a matrix
of those differences (in general terms, a matrix of distances between the
treated and control units)

```{r}
tmp <- meddat$nhAboveHS
names(tmp) <- rownames(meddat)
absdist <- match_on(tmp, z = meddat$nhTrt,data=meddat)
absdist[1:3,1:3]
abs(meddat$nhAboveHS[meddat$nhTrt==1][1] - meddat$nhAboveHS[meddat$nhTrt==0][1] )
```

## Do the match

```{r}
fm1 <- fullmatch(absdist,data=meddat)
summary(fm1, min.controls=0, max.controls=Inf )
table(meddat$nhTrt,fm1)
```

## Evaluate the design: Within set differences

Look within sets:

```{r}
meddat$fm1 <- fm1
rawmndiffs <- with(meddat, mean(nhAboveHS[nhTrt==1]) - mean(nhAboveHS[nhTrt==0]))
setdiffs <- meddat %>% group_by(fm1) %>% summarize(mneddiffs =
						   mean(nhAboveHS[nhTrt==1]) -
						   mean(nhAboveHS[nhTrt==0]),
					   mnAboveHS = mean(nhAboveHS),
					   minAboveHS = min(nhAboveHS),
					   maxAboveHS = max(nhAboveHS))

setdiffs

summary(setdiffs$mneddiffs)
quantile(setdiffs$mneddiffs, seq(0,1,.1))

```


## Evaluate the design: Compare to a randomized experiment.

```{r}
xbHS2 <- xBalance(nhTrt~nhAboveHS,
                  strata=list(nostrat=NULL,
                              hsmatch=~fm1),
                  data=meddat,report="all")
xbHS2$results
xbHS2$overall
```

## Summary of the Day

 - We can assess the randomization of a randomized experiment easily using
   covariates ($X$): compare
   the observed treatment-vs-control differences in $X$ with those consistent
   with no differences that would emerge from repeating the design.

 - How to justify an adjustment strategy for an observational study? The
   linear model adjustment strategy is difficult to justify. A stratification
   based strategy is easier to justify, inspect, learn from. (We can compare
   our stratification to a block randomized experiment, to a known design, a
   known standard.)

 - How to choose a stratification? We can do it by hand. Or we can delegate to
   a computer (i.e. `optmatch`) --- we can think of it as an optimization
   problem and ask the computer to optimize.


# End Matter

## References

