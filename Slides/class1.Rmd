---
title: |
  | Matching for Adjustment and Causal Inference
  | Class 1: Experiments, Potential Outcomes, and Treatment Effects
date: '`r format(Sys.Date(), "%B %d, %Y")`'
author: Jake Bowers
bibliography:
 - ../BIB/references.bib
fontsize: 10pt
geometry: margin=1in
graphics: yes
biblio-style: authoryear-comp
biblatexoptions:
  - natbib=true
output:
  beamer_presentation:
    slide_level: 2
    keep_tex: true
    latex_engine: lualatex
    citation_package: biblatex
    incremental: false
    template: icpsr.beamer
    includes:
        in_header:
           - defs-all.sty
    md_extensions: +raw_attribute-tex_math_single_backslash+autolink_bare_uris+ascii_identifiers+tex_math_dollars
    pandoc_args: [ "--csl", "chicago-author-date.csl" ]
colorlinks: true
header-includes:
  - \setbeameroption{hide notes}
---

<!-- To show notes  -->
<!-- https://stackoverflow.com/questions/44906264/add-speaker-notes-to-beamer-presentations-using-rmarkdown -->

```{r echo=FALSE, include=FALSE, cache=FALSE}
library(here)
source(here::here("Exercises/rmd_setup.R"))
library(tidyverse)
library(optmatch)
library(RItools)
```

# Overview and Review

## Overview of the class {.shrink}

**At the end:** 

- We estimate the TV ad caused an increase in vaccinations by 5 pct points and
  that it would be surprising to see this size of effect if the ad had no
  effect ($p=.001$).
- Recall that we compare pairs of people with the same age, so differences in
  age cannot explain the difference.
- Any differences within pair in income are smaller than we would see in an
  equivalent pair-randomized experiment.
- Also, although we did not measure party ID, and surely Republicans and
  Democrats of the same age are not equally likely to watch the ads,
  Republicans have to be 5x more likely to watch the ad than Democrats (of the
  same age) before we would estimate an effect of 0 or $p=.051$.

See also \autocite{rabb2022pnas}.

## Overview of the class {.shrink}

- We **infer** about counterfactual causal mechanisms by **estimating** and
  **testing**: *Causal inference (in this class) requires statistical
  inference.*
- Statistical inference requires **distributions** (we cannot know whether an
  estimator is unbiased without an idea of a distribution, we cannot calculate
  a $p$-value without a distribution, etc.)
- Distributions require **thought experiments** about repetition: We *imagine*
  that we could (a) re-draw random samples from a the same population or (b)
  re-assign randomized treatment. (We can also imagine reusing our priors and
  likelihood.)
- We will use the fact that distributions of certain test statistics is **known
  for randomized experiments** to (a) reason about whether we have a good
  non-randomized research design, (b) analyze outcomes, and (c) reason about
  sensitivity of results fo unobserved confounders.


So: We will start with randomized experiments even though the rest of the
course is about non-randomized studies.


## Experiments, Potential Outcomes, and Treatment Effects

  1. Causal inference in randomized experiments and the idea of only partially
     observed **potential outcomes**. The idea that we can use what we observe
     to learn about what we cannot observe.
  2. Statistical inference for causal effects in randomized experiments via the
     Fisher and Neyman approaches \autocite[Chap 2]{rosenbaum2010},
     \autocite[Chap 1-3]{gerbergreen2012}: Estimation, Estimators, Tests,
     Testing.
  3. Why are randomized experiments special? Unbiased estimators. The ability
     to assess bias. The ability to exclude alternative explanations. Tests
     with known error rates. The ability to assess error rates of tests.
  4. What does "controlling for" do in a linear model when we do not have a
     randomized experiment? How can we make the case that we are "controlling
     for" enough?

Note: You can download (and contribute to) course materials at \href{https://github.com/bowers-illinois-edu/short-course-causal}{https://github.com/bowers-illinois-edu/short-course-causal}

## Overly Ambitious Plan

 - Introduction to Jake
 - Introduction to the idea of the course: roadmap
 - Introductions to you and a project that you might work on during the week.
 - Jake introduces concepts:  potential outcomes, treatment effects, unbiased estimation, and encourages questions and answers.
 - Coffee Break
 - Questions about the lecture
 - Exercise 1: Describe your data (and update R).
 - Break
 - Jake discuss the problem of adjustment in observational studies as compared to experiments
 - Open Discussion and/or work on projects

# Concepts and Notation for Causal Inference and Statistical Inference

## Notation and Concepts for Counterfactual Causal Inference

  - *Treatment* or *Intevention*  $Z_i=1$ for treatment and $Z_i=0$ for control
    for units $i$. (We mostly assume that all units **could have** $Z_i=1$ or
    $Z_i=0$. That it is not impossible for any unit to have either value.) (Q:
    What is a unit? Examples of interventions?)

  - Each unit has a pair of *potential outcomes* $(y_{i,Z_i=1},y_{i,Z_i=0})$
    (also written  $(y_{i,1},y_{i,0})$ ) (given SUTVA).
    - Without the SUTVA assumption, and with 4 units, with two having $Z_i=1$, unit $i=1$ would have the following potential outcomes: $(y_{i,1100},y_{i,1010},y_{i,1001},y_{i,0101},y_{i,0011})$
  - *Causal Effect under SUTVA*  when $y_{i,1} \ne y_{i,0}$, $\tau_i   =
    f(y_{i,1},y_{i,0})$ ex. $\tau_i =  y_{i,1} - y_{i,0}$. (Examples of Interfering Units and Not Interfering Units)

  - *Fundamental Problem of (Counterfactual) Causality* We only see one
    potential outcome $Y_i = Z_i * y_{i,1} + (1-Z_i) y_{i,0}$ (Examples?)

  - *Covariates*,  $\bX=\begin{bmatrix} x_{11} & \ldots & x_{1k} \\ \vdots &
    \vdots & \vdots \\  x_{n1} & \ldots & x_{nk} \end{bmatrix}$ is a matrix
    containing  background information about the units that might predict
    $(y_{i,1},y_{i,0})$ or $Z$ (but that don't predict $Z$ if $Z$ is randomized
    as in an experiment).

## The observation and unobserved causal comparisons

We can learn about unobserved but theorized causal mechanisms by observing the world [@brady2008cae]:

- **Persistent association** "We always/mostly see $Y=1$ when $X=1$ and $Y=0$
  when $X=0$."
- **Counterfactual Difference** "If  $X$ had not been this value, then $Y$
  would not have been that value."
- **Difference after manipulation** "When we change $X$ from one value to
  another value, then $Y$ changes from one value to another value."
  (establishes causal priority of $X$ over $Y$, implied that $Y$ would not have
  changed.).
- **Difference after operation of a mechanism** "Once upon a time $A$ changed
  $X$, and then one day $X$ changed $B$, and because of that $B$ changed $C$,
  and finally $C$ changed $Y$."

All approaches are useful. This week we are focusing on the counterfactual approach and somewhat on the persistent association.

## How to interpret "X causes Y" in counterfactual terms?

The counterfactual approach requires that we can imagine units with and without $X$ (or say, $X=1$ and $X=0$).

 -  We can establish that X causes Y without knowing the mechanism. The
   mechanism can be complex, and it can involve probability: X causes Y
   sometimes because of A and sometimes because of B.
 - "X causes Y" can mean "With X, probability of Y is higher than would be
   without X." or "Without X there is no Y." Either is compatible with the
   counterfactual idea.
 - Of course: Correlation is not causation.
 - "X causes Y" is a statement about *what didn't happen*: "If X had not
   operated, occurred, then Y would not have occurred." (More about the
   fundamental problem of counterfactual causation later)

Today: Show that a randomized experiment allows us to **learn about**
unobserved counterfactual outcomes using statistical inference.

```{r, echo=FALSE, results="hide"}
set.seed(12345)
honeydat <- data.frame(x1=c(1,1,2,3,1,0,0,0,0,2),
    x2=sample(1:9,size=10,replace=TRUE))
honeydat$Zlat <- with(honeydat,.5*x1 + runif(10))
honeydat$Z <- as.numeric(honeydat$Z > median(honeydat$Z))
## honeydat$y0 <- with(honeydat,x1+x2+Z+rpois(10,lambda=5))
honeydat$y0 <- with(honeydat, x1 + .25*x1^2+rpois(10,lambda=8))
summary(lm(y0~x1+x2,data=honeydat))
## cor(honeydat[,c("x1","x2","Z","y0")])
honeydat$Zlat <- NULL
honeydat$y1 <- with(honeydat,y0 - c(1,1,2,3,1,0,0,0,0,2))
honeydat$Y <- with(honeydat,y1*Z + y0*(1-Z))
with(honeydat,mean(Y[Z==1]) - mean(Y[Z==0]))
honeydat$tau <- honeydat$y1 - honeydat$y0
honeydat$id <- 1:10
dat <- honeydat[,c("id","x1","x2","y0","y1","tau")]
```


## Example: Honey and Colds {.shrink}

Your friend explains a causal mechanism that eating raw honey reduces the
duration of colds. What kinds of **alternative** explanations might we come up
with for this result?

Imagine these were the underlying potential outcomes with $x_1$ and $x_2$
representing two of those explanations and that $x_1 \rightarrow y_0$ and that
$x_2 \not \rightarrow y_0$.

```{r}
kable(dat)
```

The true, unobserved, average (additive) causal effect is: `r with(honeydat, mean(y1-y0))`.

Let us run a randomized experiment and see how we do:

## An RCT:

What is happening here? How would we know whether `complete_ra` worked as it should?

```{r, echo=TRUE}
library(randomizr)
set.seed(12345)
dat$Z <- complete_ra(N=10,m=5)
dat$Y <- with(dat,Z*y1 + (1-Z)*y0)
kable(dat)
```


## Assessing randomization

We expect that the distributions of $x_1$ and $x_2$ would be (nearly) the same between the treated and control groups:

\begin{center}
\begin{tikzcd}[column sep=large]
	  Z  \arrow[from=1-1,to=1-4] &    &    & Y \\
	   x_1 \arrow[from=2-1,to=1-1] \arrow[from=2-1,to=1-4]
\end{tikzcd}
\end{center}

Here, just looking at means:
```{r, echo=TRUE}
dat %>% group_by(Z) %>% reframe(mean(x1),mean(x2))
```

```{r, echo=TRUE}
library(RItools)
bal1 <- balanceTest(Z~x1+x2,data=dat)
bal1$results[,1:4,]
```

## What **should** we expect from an experiment?

...in regards covariate balance? Lets simulate to learn:

```{r, echo=TRUE}

new_exp <- function(Z){
	newZ <- sample(Z)
	return(newZ)
}

diff_means <- function(x,Z){
	mean(x[Z==1]) - mean(x[Z==0])
}

all_cov_bal<- replicate(1000,diff_means(x=dat$x1,Z=new_exp(dat$Z)))

summary(all_cov_bal)

obs_cov_bal <- diff_means(dat$x1,dat$Z)
obs_cov_bal

```





## Estimating the ATE in an RCT:

Here are two proposals for estimating the ATE. How would we know whether either
or both of them work well? (What do we want estimators to do for us?)

```{r echo=TRUE}
est1 <- function(Z,Y){
    mean(Y[Z==1]) - mean(Y[Z==0])
}
est2 <- function(Z,Y){
    coef(lm(Y~Z))[["Z"]]
}

with(dat,est1(Z=Z,Y=Y))
with(dat,est2(Z=Z,Y=Y))
```

## How does randomization help us trust our estimators?

```{r, echo=TRUE}
## The truth:
with(dat,mean(y1-y0))

new_exp <- function(Z){
    ## This next shuffles Z
    newZ <- sample(Z)
    return(newZ)}

new_est <- function(newZ,y0,y1,the_est){
    newY <- newZ * y1 + (1-newZ)*y0
    result <- the_est(Z=newZ,Y=newY)
}

set.seed(1235)
dist_est1 <- with(dat, replicate(100, new_est(newZ=new_exp(Z),y0=y0,y1=y1,the_est=est1)))
mean(dist_est1)
dist_est1 <- with(dat, replicate(1000, new_est(newZ=new_exp(Z),y0=y0,y1=y1,the_est=est1)))
mean(dist_est1)
dist_est1 <- with(dat, replicate(10000, new_est(newZ=new_exp(Z),y0=y0,y1=y1,the_est=est1)))
mean(dist_est1)
```

## How does randomization help us trust our estimators?

What did randomization provide here?

 1. Grounds for repetition (i.e. we **knew** how to repeat the generation of $Z$),
 2. No need to mention `x1` (we could check to see if we should worry about x1).
 3. an unbiased estimator.

## What about if we didn't know exactly how Z was assigned?

Imagine that `x1` causes `Z` (here, Z is randomized but x1 changes Z before revealing y0 or y1):

```{r biasexample, cache=TRUE, echo=TRUE}
new_exp2 <- function(Z,x1){
    newZ1 <- sample(Z)
    # newZ <- newZ1*rbinom(10,size=1,prob=(x1+1)/4)
    newZ <- pnorm(x1+newZ1)>pnorm(median(x1+newZ1))
    return(as.numeric(newZ))
}
with(dat,mean(y1-y0))
with(dat,est1(new_exp2(Z,x1),Y))

set.seed(1235)
dist_est1a <- with(dat, replicate(10000, new_est(newZ=new_exp2(Z,x1),y0=y0,y1=y1,the_est=est1)))
summary(dist_est1a)
## And recall our previous distribution of our estimator across randomizations
summary(dist_est1)
```

## What about if we didn't know exactly how Z was assigned?

Imagine that `x1` causes `Z` (here, Z is randomized but x1 changes Z before reveals y0 or y1):

```{r}
plot(density(dist_est1),ylim=c(0,.8))
lines(density(na.omit(dist_est1a)),col="blue")
abline(v=with(dat,mean(y1-y0)))
points(c(mean(dist_est1a,na.rm=TRUE),mean(dist_est1)),c(0,0),pch=c(17,18),cex=2,col=c("blue","black"))
```

## What about if we didn't know how Z was assigned at all?

How would we assess bias if we didn't know that x1 caused Z?

 - We cannot simply shuffle Z. Because we don't know how Z arose.
 - Do we know how x1 was generated? If so, we could re-generate x1 and *hope* that our x1 to Z function is right
 - We could repeatedly re-generate Y itself if we **knew** how it was created.
 - We could resample the entire dataset if we **knew** how it was sampled.

## What about controlling for x1?

If we knew that x1 causes both Z and Y, then perhaps we can "control for it"? What does this do?

```{r echo=TRUE}
set.seed(12345)
dat$Z <- new_exp2(dat$Z,dat$x1)
e1 <-lm(Y~Z,data=dat)
e2 <- lm(Y~Z+x1,data=dat)
coef(e1)[["Z"]]
coef(e2)[["Z"]]
```

Recall the true ATE:

```{r}
with(dat,mean(y1-y0))
```


## What about controlling for x1?

What does it mean when we "control for" x1?

```{r}
with(dat,plot(x1,Y,pch=c(1,19)[Z+1],col=c("black","grey")[Z+1]))
abline(a=coef(e2)[[1]],b=coef(e2)[["x1"]])
abline(a=coef(e2)[[1]]+1,b=coef(e2)[["x1"]])
text(y=c(coef(e2)[[1]],coef(e2)[[1]]+1),x=c(0,0),labels=c("Z=0","Z=1"))
```

## What about controlling for x1?

Controlling for as residualization:

```{r echo=TRUE}
resid1 <- residuals(lm(Y~x1,data=dat))
resid2 <- residuals(lm(Z~x1,data=dat))
e2new <- lm(resid1~resid2)
coef(e2new)[[2]]
stopifnot(all.equal(coef(e2new)[[2]], coef(e2)[["Z"]]))
```

```{r out.width=".5\\textwidth"}
par(mfrow=c(1,2),oma=rep(0,4),mar=c(3,3,1,1),mgp=c(1.25,.5,0))
with(dat,plot(x1,Y))
abline(lm(Y~x1,data=dat))
plot(dat$x1,resid1)
abline(lm(resid1~dat$x1))
```


## What about controlling for x1?

Controlling for as residualization:

```{r out.width=".8\\textwidth"}
par(mfrow=c(1,2),oma=rep(0,4),mar=c(3,3,1,1),mgp=c(1.25,.5,0))
with(dat,plot(x1,Y))
abline(lm(Y~x1,data=dat))
plot(dat$x1,resid1)
abline(lm(resid1~dat$x1))
```

## What about controlling for x1?

**How** should we control for x1? How to choose?

```{r echo=TRUE}
set.seed(12345)
dat$Z <- new_exp2(dat$Z,dat$x1)
e1 <-lm(Y~Z,data=dat)
e2 <- lm(Y~Z+x1,data=dat)
e3 <- lm(Y~Z+x1+I(x1^2),data=dat)
e4 <- lm(Y~Z*x1+I(x1^2),data=dat)
coef(e1)[["Z"]]
coef(e2)[["Z"]]
coef(e3)[["Z"]]
preddat <- expand.grid(Z=unique(dat$Z),x1=unique(dat$x1))
preddat$e4fit <- predict(e4,newdata=preddat)
## Equal weight estimate
with(preddat,mean(e4fit[Z==1]) - mean(e4fit[Z==0]))
```

Is this enough? What else might we do? Did we do a good job? How would we know? What does "controlling for" really mean?


# Summary and Overview

## Benefits of Randomized Designs

Randomization makes $y_1, y_0, \bX \perp Z$. How to make use of this fact in a randomized experiment?

 1. Interpretable comparisons (lack of omitted variable bias, confounding,
    selection bias)
      - Can I interpret differences in outcome as caused by $Z$ and not $X$?
	Is it easy to confuse the effect of $Z$  with the  effects of $X$?
      - How does  randomization do this? How  does randomization eliminate
	**alternative explanations**?
 2. Reliable statistical inferences (estimators and tests)
      - The idea of **design-based** versus **model-based** statistical
	inference (next few slides).

## Design Based Approach: Estimate Averages

  1. Notice that the observed $Y_i$ are a sample from  the (small, finite)
     population of $(y_{i,1},y_{i,0})$.
  2. Decide to focus on the average, $\bar{\tau}$, because sample averages,
     $\hat{\bar{\tau}}$ are unbiased and consistent estimators of population
     averages under random sampling (where no covariate determines the sample
     inclusion probability or assignment to $Z_i$).
  3. Estimate $\bar{\tau}$ with the observed difference in means.

\centering
  \includegraphics[width=.5\textwidth]{images/cartoonNeyman.pdf}

## Design Based Approach: Estimate Averages

\centering
  \includegraphics[width=.9\textwidth]{images/cartoonNeyman.pdf}


## Design Based Approach: Estimate Averages


```{r}
set.seed(12345)
dat$Z <- randomizr::complete_ra(N=10,m=5)
dat
## The estimand: the ATE
with(dat,mean(y1-y0))
## An estimator
with(dat,est1(Z,Y))
```

## What about when we have not randomized?

We can try to adjust (for example by "controlling for". But adjustment raises new questions: how to adjust? how to assess our adjustment?)

Ideas for a simpler way to say "We have held x1 constant?" as a way to discard alternative explanations based on x1?

## Lingering Questions?

# Appendix

## Design Based Approach: Test Hypotheses

 1. Make a guess about $\tau_i$.
 2. Then measure surprise or consistency of data with this guess given the
    design. (Given all of the ways this experiment could have  occurred, how
    many look more extreme than what we observe? Does our observation look
    typical or rare?)

\centering
  \includegraphics[width=.6\textwidth]{images/cartoonFisherNew1.pdf}

## Design Based Approach: Test Hypotheses

\centering
  \includegraphics[width=\textwidth]{images/cartoonFisherNew1.pdf}

## Design Based Approach: Test Hypotheses

\centering
  \includegraphics[width=\textwidth]{images/cartoonFisherNew2.pdf}

## Approaches to creating interpretable comparisons:

   - Randomized experiments (more precision from reducing heterogeneity in $Y$)
   - Instrumental variables (with randomized $Z$ created $D$)
   - Natural Experiments / Discontinuities (one $X$ creates $Z$) (includes RDD)
   - Difference-in-Differences (reduce bias *and* increase precision from reducing heterogeneity in $Y$)
   - Semi-/Non-parametric Covariance Adjustment (ex. Matching)
   - Parametric covariance adjustment

## References

