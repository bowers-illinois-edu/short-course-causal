---
title: |
  | Matching for Adjustment and Causal Inference
  | Class 2: Controlling For, Holding Constant, Assessing Stratifications
date: '`r format(Sys.Date(), "%B %d, %Y")`'
author: Jake Bowers
bibliography:
 - ../BIB/references.bib
fontsize: 10pt
geometry: margin=1in
graphics: yes
biblio-style: authoryear-comp
biblatexoptions:
  - natbib=true
output:
  beamer_presentation:
    slide_level: 2
    keep_tex: true
    latex_engine: lualatex
    citation_package: biblatex
    incremental: true
    template: icpsr.beamer
    includes:
        in_header:
           - defs-all.sty
    md_extensions: +raw_attribute-tex_math_single_backslash+autolink_bare_uris+ascii_identifiers+tex_math_dollars
header-includes:
  - \setbeameroption{hide notes}
---

<!-- To show notes  -->
<!-- https://stackoverflow.com/questions/44906264/add-speaker-notes-to-beamer-presentations-using-rmarkdown -->

```{r echo=FALSE, include=FALSE, cache=FALSE}
library(here)
## You may need to change this next line to the following if you are using the EIM repo
## source(here::here("rmd_setup.R"))
source(here::here("Exercises/rmd_setup.R"))
library(tidyverse)
library(optmatch)
library(RItools)
library(estimatr)
```

# Overview and Review

## Controlling for, Holding Consant, Evaluating Stratifications/Designs

  1. **Last time:**
     1. An intervention or causal agent has a counterfactual causal effect on a
        unit $i$ if its potential outcomes differ: $y_{i,1} \ne y_{i,0}$.
     2. We cannot directly see **both** potential outcomes.
     3. So we have three options: we can estimate average effects, or (not last
        time) test hypotheses about effects, or predict individual level
        effects (and their posterior distributions, usually).
     4. We can learn about the true, underlying, unobservable, average
        treatment effect (the ATE) in a way that is not systematically wrong
        **if we have a randomized experiment (RCT)** using the sample
        differences of means.
    5. If we do not have an RCT then we have a series of worries: (a) how to
       manage alternative explanations, (b) how much bias is there in our
       estimator of the underlying ATE? (How to tell how much bias there is?)
    6. If we use a linear model to "control for" a covariate then a new set of
       questions arises:
       1. How much is our adjustment based on functional form assumptions and
          how much is supported by the data?
       2. Related: how much extrapolation is involved? how much interpolation?
          how can we justify one specification over another?
       3. Overall: how can we claim that we have "controlled for" enough? What
          is the standard against which we can compare a given linear model
          adjustment strategy?
  2. **Today:** Confront those question and use stratification rather than
     residualization: actively and transparently "hold constant" rather than
     "remove linear additive relationships". Also, assess the success of
     stratification and maybe stratify optimally.

## Overly Ambitious Plan

  - 00:00 -- 00:30  --- Review
  - 00:30 -- 01:30 ---  Lecture by Jake with lots of questions from the class
  - 01:30 -- 01:40 --- Break
  - 01:40 -- 02:00 --- Questions about the lecture and/or readings
  - 02:00 -- 03:00 --- Break and Exercise 2 and maybe Exercise 3.
  - 03:00 -- 04:00 --- Discussion of questions arising from the exercises and
    questions lingering from reading, or lecture, or open discussion on any
    topic.

## "Controlling For" versus "Holding Constant"

These two phrases imply differ adjustments:

```{r}
load(url("http://jakebowers.org/Data/meddat.rda"))
## These next are equivalent ways to get rates per 1000 from counts
## meddat$HomRate03<-with(meddat, (HomCount2003/Pop2003)*1000)
## meddat$HomRate08<-with(meddat, (HomCount2008/Pop2008)*1000)
meddat<-transform(meddat, HomRate03=(HomCount2003/Pop2003)*1000)
meddat<-transform(meddat, HomRate08=(HomCount2008/Pop2008)*1000)
```

Controlling for:
```{r echo=TRUE, results="markup"}
lm1 <- lm(HomRate08~nhTrt+nhAboveHS,data=meddat)
coef(lm1)
```

Holding constant:
```{r echo=TRUE}
lm2a <- lm(HomRate08~nhTrt,data=meddat,subset=nhAboveHS>.05)
lm2b <- lm(HomRate08~nhTrt,data=meddat,subset=nhAboveHS<=.05)
coef(lm2a)
coef(lm2b)
```

## Detail: How to estimate an overall ATE while holding constant?

We know how to analyze a block-randomized (or strata-randomized) experiment
(see @gerbergreen2012)): each block is a mini-experiment. We estimate the ATE
within each block and combine by weighting each block.

```{r}
## Make a 2 category variable:
meddat$nhAboveHS2cat <- (meddat$nhAboveHS>.05)

tmp <- meddat %>% group_by(nhAboveHS2cat) %>% summarize(ateb = mean(HomRate08[nhTrt==1]) - mean(HomRate08[nhTrt==0]),
   prob_trt = mean(nhTrt),
   nbwt = n()/nrow(meddat),
   prec_wt = nbwt * prob_trt * ( 1 - prob_trt))

est_ate1 <- with(tmp,sum(ateb*nbwt))
est_ate2 <- with(tmp,sum(ateb*prec_wt/(sum(prec_wt))))

## Now at the individual level
meddat <- meddat %>% group_by(nhAboveHS2cat) %>% mutate(nb=n(),
    mb=sum(nhTrt),
    ateb = mean(HomRate08[nhTrt==1]) - mean(HomRate08[nhTrt==0]),
   prob_trt = mean(nhTrt),
   nbwt = ( nhTrt/prob_trt) + (1-nhTrt)/(1 - prob_trt),
   prec_wt = nbwt * prob_trt * ( 1 - prob_trt))

## Two ways to use the block-size weight
est_ate1a <- difference_in_means(HomRate08~nhTrt,blocks=nhAboveHS2cat,data=meddat)
est_ate1b <- lm_robust(HomRate08~nhTrt,weights=nbwt,data=meddat)

## Three other ways to use the precision or harmonic weight
est_ate2a <- lm_robust(HomRate08~nhTrt+nhAboveHS2cat,data=meddat)
est_ate2b<- lm_robust(HomRate08~nhTrt,fixed_effects=~nhAboveHS2cat,data=meddat)
est_ate2c<- lm_robust(HomRate08~nhTrt,weights=prec_wt,data=meddat)
```

## Did the stratification adjust enough?

What is the standard for "adjusted enough"? How about an experiment where
treatment has been randomized within each block? How does our adjustment
compare to that standard?

```{r simpsim, cache=TRUE}
new_exp_and_test_stat <- function(data){
    ## Shuffle intervention within block
    dat <- data %>% group_by(nhAboveHS2cat) %>% mutate(newZ = sample(nhTrt))
    ## here, using the precision weighted version
    test_stat_hyp <- coef(lm(nhAboveHS~newZ+nhAboveHS2cat,data=dat))[["newZ"]]
    return(test_stat_hyp)
}

no_effects_dist <- replicate(10000,new_exp_and_test_stat(meddat))
obs_test_stat <- coef(lm(nhAboveHS~nhTrt+nhAboveHS2cat,data=meddat))[["nhTrt"]]
pval <- 2*min( mean(no_effects_dist >= obs_test_stat), mean(no_effects_dist <= obs_test_stat))
```

```{r}
plot(density(no_effects_dist),main="Block-Rand Experiment No Effects Dist (plus observed effects)")
abline(v=obs_test_stat)
```

A faster way to do the preceding (to compare what we observe to the distribution of the mean difference if there were no effects and the design was block-randomized):

```{r xbal, echo=TRUE}
xb1 <- xBalance(nhTrt~nhAboveHS,strata=list(unstrat=NULL,HS=~nhAboveHS2cat),data=meddat,report="all")
xb1$results
xb1$overall
```

Outcome analysis:

```{r xbal, echo=TRUE}
xb2 <- xBalance(nhTrt~HomRate08,strata=list(unstrat=NULL,HS=~nhAboveHS2cat),data=meddat,report="all")
xb2$results
xb2$overall
## FYI: when we are thinking about estimators versus test statistics
## lm3 <- lm_robust(HomRate08~nhTrt,fixed_effects = ~nhAboveHS2cat,data=meddat)
```







## References

