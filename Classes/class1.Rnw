
% For LaTeX-Box: root = class1.tex 
% search and replace keep.comment=FALSE to TRUE for commented version
\documentclass[10pt,letterpaper]{article}
\usepackage{fontspec}
\usepackage{ulem}

\title{Session 1 --- Matching Short Course}

\author{Jake Bowers}

\usepackage{../Styles/ps531}


\includeversion{comment}
\markversion{comment}
%\excludeversion{comment}

\begin{document}
\normalem

<<include=FALSE,cache=FALSE>>=
opts_chunk$set(tidy=TRUE,echo=TRUE,results='markup',strip.white=TRUE,fig.path='figs/fig',cache=FALSE,highlight=TRUE,width.cutoff=132,size='footnotesize',out.width='1.2\\textwidth',message=FALSE,comment=NA)
@

\maketitle

\begin{enumerate}
    \setcounter{enumi}{-1}
  \item Today we'll work with a dataset that documents a public policy
    intervention. In 2004 the municipality of Medell\'{i}n, Columbia built
    built the first line of the Metrocable --- a set of cable cars that
    connected poor neighborhoods on the edges of the city to the center of the
    city \citep{cerda2012reducing}. Professor Magdalena Cerd\'{a} and her collaborators asked whether
    this kind of integration could have an effect on violence in these poor
    (and heretofore violent) neighborhoods. We have some of the data from this
    project to use here.

    The data Cerd\'{a} collected tell us about the roughly 50 neighborhoods in
    the study, about half of which had access to the Metrocable line and half
    did not. You can load the data here:

%    {\centering
%    \includegraphics[width=.5\textwidth]{MedellinNeighborhoods.pdf}
%    }

    
<<>>=
load("../Resources/Cerdaetal/meddat.rda")
@

I don't have a formal codebook but here are my guesses about the meanings of
some of the variables:

\begin{Verbatim}
## Some Covariates
nh03         Neighborhood id
nhGroup      Treatment (T) or Control (C)
nhTrt        Treatment (1) or Control (0)
nhHom        Mean homicide rate per 100,000 population in 2003
nhDistCenter Distance to city center (km)
nhLogHom     Log Homicide (i.e. log(nhHom))

## Outcomes (BE03,CE03,PV03,QP03,TP03 are baseline versions)
BE      Neighborhood amenities Score 2008
CE      Collective Efficacy Score 2008
PV      Perceived Violence Score 2008
QP      Trust in local agencies Score 2008
TP      Reliance on police Score 2008
hom     Homicide rate per 100,000 population Score 2008-2003 (in log odds)
\end{Verbatim}

Let us first do a very simple analysis to get concepts clear. 

I will write $Z_i=1$ to mean that a neighborhood $i$ was assigned
intervention 1 (in this case, a Metrocable station), $y_{i,Z=1} \equiv y_{i,1}$
to refer to the \emph{potential outcome} for a neighborhood assigned
intervention and $y_{i,Z=0} \equiv y_{i,0}$ to refer to the counterfactual
condition in which neighborhood $i$ did not receive the intervention.

I will also say that the Metrocable intervention had a \emph{causal effect} if
$y_{i,1} \ne y_{0,i}$ (and no causal effect if $y_{i,1} = y_{i,0}$).

We tend to have two ways to use observed data ($Z$, the observed assignment of
the intervention), and $Y$ (the observed outcome), to give us a hint about the
relationship between $y_{i,1}$ and $y_{i,0}$ (and thus to give us a hint about
causal effects. I say, "hint", because, the fact that \emph{we cannot know the true
relationship between potential outcomes} is often called the "Fundamental
problem of causal inference" \citep{holland:1986a,brady2008cae}.

First, and simplest, we can say, "We do not know the true relationship between
potential outcomes, but we can articulate a hypothesis about that relationship
and then test this hypothesis." \cite{fisher:1935, rosenbaum2010design}. This is
what I call the "testing approach."

Second, and perhaps most commonly, we can say, "We do not know the true
relationship at the individual level of $y_{i,1}$ and $y_{i,0}$, but we can
estimate the relationship between averages of them ($\bar{y}_{1}$ and
$\bar{y}_{0}$) \cite{neyman:1923,angrist2009mostly}. I call this the
"estimation approach."

Just to be clear, here are the observed $Z_i$ and $Y_i$ for for a few of the
Medell\'{i}n neighborhoods:

<<>>=
library(xtable)

set.seed(20140714)
tmp<-meddat[sample(1:nrow(meddat),10),c("nhTrt","PV")]
y1<-with(tmp,ifelse(nhTrt==1,PV,NA))
y0<-with(tmp,ifelse(nhTrt==0,PV,NA))
theTab<-cbind(tmp,y1,y0)
theXTab<-xtable(theTab,digits=2,display=c("d","d","f","f","f"))

@

This table shows that treatment assignment reveals one potential outcome per
unit to us, but not both.

<<results='asis'>>=
print(theXTab,floating=FALSE)
@



Let us start with estimation.

\item Estimate and interpret the average effect of the intervention on one of the outcomes
  of your choice. If you are using \texttt{hom}, I would advise using
  \texttt{exp(hom)} to get back to the original scale.


  \begin{comment}

<<>>=

atePV<-lm(PV~nhTrt,data=meddat)
atePV

@

\end{comment}

\item What does this number have to do with $y_{i,1}$ and $y_{0,i}$? (Maybe we
  should discuss this as a group.) Here are some hints and ideas to help
  orient discussion. Below the math, I make the same demonstration by
  simulation. One guiding intuition is that the potential outcomes that we
  observe can be thought of as a random sample of the finite population of
  potential outcomes (i.e. the 25 $y_{i,1}$ revealed to us by randomized
  treatment assignment are a random sample of the population of 48 $y_{i,1}$.)
 
    \citet{neyman:1923} proposed (1) that we decide to stop focusing on the
    individual level causal effects and instead be interested in $\bar{\tau}=
    (1/n) (\sum_{i=1}^n y_{1i}) - (1/n) (\sum_{i=1}^n y_{0i}) = \bar{r}_{1} -
    \bar{r}_{0}$ (i.e. he suggested we choose to care about mean differences
    of the counterfactuals rather than any individual counterfactuals
    themselves) and (2) he claimed that $\hat{\bar{\tau}}=(1/m)\sum_{i=1}^n
    Z_i Y_i - (1/(n-m))\sum_{i=1}^n (1-Z_i) Y_i$ was a good \textbf{estimator}
    of $\bar{\tau}$ (where $Y_i$ is observed outcomes and relates to potential
    outcomes via $Y_i=Z_i y_{1i}+(1-Z_i) y_{0i}$, $m$ is number of treated
    observations and $n$ is total number of observations). We are going to
    prove this is the case, for a specific understanding of ``good''.   
 
    Neyman, chose the idea of ``unbiasedness'' for ``good''.
    An unbiased estimator is an estimator (i.e. a formula that when
    repeatedly applied to the data (across samples, across
    assignments, etc..) produces a distribution --- a sampling
    distribution, a randomization distribution), where the mean of
    this distribution is the ``true'' value in the ``population'' (or
    the experimental pool). In Neyman's example, the population is the
    set of agricultural fields to which he wants to assign fertilizer
    treatments. For us, it is these 48 neighborhoods. We can think of
    experimental pools as ``finite populations.'' We can write this
    overly wordy definition concisely like this:
  $$E_{Y}[\hat{\bar{\tau}}]=\bar{\tau},$$
  where $E_{Y}$ (the ``expected value'') is ``the average across all
  ways to draw assignments, $Z$, from the urn.'' You'll mostly just
  see something like $E[\hat{\beta}]=\beta$ as a statement that
  whatever formula is giving you $\hat{\beta}$ is, on average across
  imaginary replications, equal to $\beta$ --- is unbiased. Now, the
  expectation operator, $E$ has a kind of algebra associated with it
  such that $E[aX]=aE[X]$ if $X$ is something that can vary and $a$ is
  some constant, or $E[X+Y]=E[X]+E[Y]$ if $X$ and $Y$ both vary
  independently (or $E[aX_1+aX_2+\ldots]=a(E[X_1]+E[X_2]+\ldots)$).
  
  Here is an incomplete version of the derivation that shows that, in
  fact, the observed sample difference of means can tell us something
  about the unobserved population difference of means, and, in fact,
  will be, on average, equal to the unobserved population difference
  of means: i.e. that the sample difference of means is an unbiased
  estimator of the population difference of means. 
  
  We want to show that $E_{Y}[\hat{\bar{\tau}}]=\bar{\tau}$. Here only
  $Z_i$ is random. And, I can tell you that $E_{Y}[Z_i]=m/n$ (what
  does this mean?). I use $E_R$ at first just to be clear that we are
  taking expectations over the possible randomizations. Mostly you'll
  just see people use $E$ without a discussion of the space over which the expectations are to be taken.
  
  
     
  \begin{align}
    E_{Y}\left[ \hat{\tau} \right]&=E_{Y}\left[ \sum_{i=1}^n Z_i \frac{Y_{i}}{m}-\sum_{i=1}^n (1-Z_i) \frac{Y_{i}}{n-m} \right] \\
    \intertext{recall that $Y_{i}=Z_i y_{1i} + (1-Z_i) y_{0i}$ or $Y_i=y_{0i}$ when $Z_i=0$, etc.}
    &=E \left[ \sum_{i=1}^n Z_i \frac{y_{1i}}{m} \right]-E \left[ \sum_{i=1}^n (1-Z_i) \frac{y_{0i}}{n-m} \right] \\
    \intertext{recall that only $Z$ is random and that $E_R[Z_i]=m/n$ in this simple design}
    &=\sum_{i=1}^n \frac{m}{n} \frac{y_{1i}}{m}-\sum_{i=1}^n  \left( 1-\frac{m}{n} \right) \frac{y_{0i}}{n-m} \\
    \intertext{since $(1-(m/n)=(n-m)/n$ }
    &=\sum_{i=1}^n \frac{y_{1i}}{n}-\sum_{i=1}^n \frac{y_{0i}}{n} \\
    &=\bar{r}_{1i}-\bar{r}_{0i}=\bar{\tau}
  \end{align}

  

  

\item Here is the same demonstration by simulation. Let's talk about what is
  going on here. Why isn't the mean of the distribution of the means exactly
  the same as the truth?

<<cache=TRUE>>=

y0<-meddat$PV ## we could have also use y0<-rnorm(48) or something else
y1<-y0+1


newexperiment<-function(origZ,y1,y0){
  Znew<-sample(origZ)
  Y<-Znew*y1+(1-Znew)*y0
  epthat<-mean(Y[Znew==1] - mean(Y[Znew==0]))
  return(epthat)
}


## set.seed(123456) ## if we uncomment set.seed, everyone should have the
## exact same answer
randdist<-replicate(1000,newexperiment(origZ=meddat$nhTrt,y1=y1,y0=y0))

summary(randdist)

@

What is this a plot of?

<<out.width='.5\\textwidth'>>=
plot(density(randdist))
rug(randdist)
rug(1,ticksize=.3,lwd=2,col="blue")
rug(mean(randdist),ticksize=.3,lwd=2,col="red")

@


\item Now, the Metrocable intervention was not a randomized experiment. And in
  most social science papers, we would tend to see something like the
  following:


<<>>=

newformula<-reformulate(c("nhTrt",names(meddat)[4:25]),response="PV")
lm1<-lm(newformula,data=meddat)
xtablm1<-xtable(lm1,digits=2)
@

<<results='asis'>>=
print(xtablm1,floating=FALSE)
@

Why would someone report the effect shown in the table above instead of the
simple effect that we calculated before? What are some of the strengths and
weaknesses of this approach to \emph{statistical adjustment}?


\item If we have extra time, we can explore the testing approach to
  statistical inference for causal effects. Rather than change the quantity of
  interest, \citet[Chap 2]{fisher:1935} showed us how to assess hypotheses
  about the individual level causal effects. Specifically, that, even if one
  cannot know how $y_{i,1}$ and $y_{i,0}$ relate for any given individual, one
  can ask questions (or make claims) about this relationship and then use the
  data to assess the evidence against the such hypotheses. His idea was that
  we could use $p$-values to summarize the evidence against a hypothesis of
  this sort. Now, what do you think it would mean for us to write a null
  hypothesis about unobserved potential outcomes  $H_0: y_{i,1}=y_{i,0}$ and
  then to say, $p=.01$? What would it mean for us to say $p=.3$? That is,
  imagine that someone said, "Given this experiment, how plausible would it be
  to observe this particular value if, in fact, there were no effect on any of
  the neighborhoods in the study?" And that the answer was $p=.01$?

\item A $p$-value is a probability and thus requires a probability
  distribution --- and specifically requires a probability
  distribution to characterize the situation posited by the null
  hypothesis. Here, the null hypothesis is $H_0: y_{i,1}=y_{i,0}$ for
  all $i$, although other null hypotheses will be useful in other
  circumstances.  In frequentist statistics a probability distribution
  requires that some physical act has been repeated: for example, the
  probability that a flipped coin is heads can be defined as the long-run
  proportion of heads across repeated flips. So, what repetition does the $p$
  refer to in an experiment?
  
\item  We want to
  generate a distribution that tells us all of the ways the experiment could
  have turned out under the null hypothesis. We have a sense that we are
  talking about repeating the experiment.   \citet[Chap
  2]{rosenbaum2010design} explains that we can generate this distribution
  because we know that $Y_i$ is a function of what we do not observe:
   
  \begin{equation}
    Y_i=Z_i y_{i,1} + (1-Z_i) y_{i,0} \label{eq:obsandpot}
  \end{equation}
 
  So, if $y_{i,0}=y_{i,0}$ then $Y_i=y_{i,0}=y_{i,1}$.

  Now, what does it mean to repeat the experiment? What is happening below?

<<>>=

newTestStat<-function(Y,Z){
  Znew<-sample(Z)
  mean(Y[Znew==1])-mean(Y[Znew==0])
}

nullDist<-replicate(1000,newTestStat(Y=meddat$PV,Z=meddat$nhTrt))

obsTestStat<-with(meddat,mean(PV[nhTrt==1])-mean(PV[nhTrt==0]))

oneSidedP<-mean(nullDist<=obsTestStat)

oneSidedP

newTestStatRank<-function(Y,Z){
  Znew<-sample(Z)
  rankY<-rank(Y)
  mean(rankY[Znew==1])-mean(rankY[Znew==0])
}

nullDistRank<-replicate(1000,newTestStatRank(Y=meddat$PV,Z=meddat$nhTrt))

obsTestStatRank<-with(meddat,{rankPV<-rank(PV);
		      mean(rankPV[nhTrt==1])-mean(rankPV[nhTrt==0])})

nullDistRank<-replicate(1000,newTestStatRank(Y=meddat$PV,Z=meddat$nhTrt))

oneSidedPRank<-mean(nullDistRank<=obsTestStatRank)

oneSidedPRank

@


\end{enumerate}





\bibliographystyle{apalike}
\bibliography{/Users/jwbowers/Documents/PROJECTS/BIB/big}




\end{document}
