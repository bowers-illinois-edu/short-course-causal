

% For LaTeX-Box: root = class4.tex
% search and replace keep.comment=FALSE to TRUE for commented version
\documentclass[10pt,letterpaper]{article}
%\usepackage{fontspec}
\usepackage{ulem}

\title{Session 4 --- Matching Short Course}

\author{Jake Bowers}

\usepackage{../Styles/ps531}
\newcommand{\teeW}{\ensuremath{t_{W}}}
\newcommand{\pcor}{\ensuremath{\rho_{y \cdot w|z\mathbf{x}}}}

\includeversion{comment}
\markversion{comment}
%\excludeversion{comment}

\begin{document}
\normalem

<<include=FALSE,cache=FALSE>>=
opts_chunk$set(tidy=TRUE,echo=TRUE,results='markup',strip.white=TRUE,fig.path='figs/fig',cache=FALSE,highlight=TRUE,width.cutoff=132,size='footnotesize',out.width='1.2\\textwidth',message=FALSE,comment=NA)

options(width=110,digits=3)
@

\maketitle

\begin{enumerate}
    \setcounter{enumi}{-1}
  \item  We'll continue to work with the Cerd\'{a} et al data. We completed
    some statistical and conceptual preliminaries on Monday. On Tuesday, we
    started work on making simple matched designs after we confronted some of
    the problems with the use of linear models for statistical adjustment.
    Yesterday, we matched on many variables at once using scores (like the
    Mahalanobis distance score or the propensity score) and struggled with
    some of the problems that score estimation raises, then we used calipers to
    allow the matching algorithm to choose to delete some observations and
    spent some time learning about the observations that the algorithm chose
    to delete, and finally we learned how to estimate an average treatment
    effect (and made some experiments to figure out which proposals for effect
    estimation are unbiased). We even spoke a bit about M-bias and choice of
    covariates for adjustment.

    Today we will create confidence intervals and calculate $p$-values for our
    estimates and also work on one approach to sensitivity analysis.

<<>>=
load(url("http://jakebowers.org/Matching/meddat.rda"))
## or, if that doesn't work
## meddat<-read.csv(url("http://jakebowers.org/Matching/meddat.csv"))
@


Before we begin you should create the outcome variables and useful formulas
<<>>=
meddat$HomRate03<-with(meddat, (HomCount2003/Pop2003)*1000)
meddat$HomRate08<-with(meddat, (HomCount2008/Pop2008)*1000)
newformula<-reformulate(c("nhTrt",names(meddat)[5:24],"HomRate03"),response="HomRate08")
balfmla<-update(newformula,nhTrt~.-nhTrt-nhDistCenter)
@

\item Let's start with your favorite matched design from last time. Here is
  mine (not really a favorite, but a matched design that I can use for
  demonstration.)

<<>>=
## install.packages("glmnet",dependencies=TRUE)
library(glmnet)
library(RItools)
library(optmatch)

## Scalar distance on baseline outcome
tmp <- meddat$HomRate03
names(tmp) <- rownames(meddat)
absdist <- match_on(tmp, z = meddat$nhTrt)
X<-model.matrix(update(balfmla,.~.-1),data=meddat)
y<-meddat$nhTrt
set.seed(12345)
ridge1cv<-cv.glmnet(X,y,family="binomial",alpha=0,type.measure="class")
pScore<-predict(ridge1cv,newx=X,s="lambda.1se")[,1]
stopifnot(all.equal(names(pScore),row.names(meddat)))
meddat$pScore<-pScore
psDist<-match_on(nhTrt~pScore,data=meddat)

##Other pscores (may be faster and easier)
## install.packages("brglm",dependencies=TRUE)
library(brglm)
brglm1<-brglm(balfmla,data=meddat,family=binomial)
pScore2<-predict(brglm1)
meddat$pScore2<-pScore2
psDist2<-match_on(nhTrt~pScore2,data=meddat)

## Overlapping propensity score plot
## par(mfrow=c(1,2))
## with(meddat,boxplot(split(pScore2,nhTrt)))
## with(meddat,boxplot(split(pScore,nhTrt)))

## Mahalanobis distance
meddatRanks<-as.data.frame(sapply(meddat[,all.vars(balfmla)],rank))
row.names(meddatRanks)<-row.names(meddat)
meddatRanks$nhTrt<-meddat$nhTrt

mahalDist<-match_on(balfmla,data=meddat)

## Match on mahalanobis distance plus two calipers
### Use the calipers to exclude really bad matches.
quantile(as.vector(mahalDist),seq(0,1,.1))
quantile(as.vector(psDist),seq(0,1,.1))
quantile(as.vector(psDist2),seq(0,1,.1))
quantile(as.vector(absdist),seq(0,1,.1))

fm1<-fullmatch(mahalDist+caliper(psDist2,3),data=meddatRanks,tol=.00001)
fm2<-fullmatch(psDist2+caliper(psDist2,3),data=meddat,tol=.00001,min.controls=.25,max.controls=4)
pm2<-pairmatch(psDist2,data=meddat,tol=.00001,remove.unmatchables=TRUE)
## pm2<-pairmatch(mahalDist+caliper(psDist2,4),data=meddat,tol=.00001,
##		remove.unmatchables=TRUE)
## fm2a<-fullmatch(psDist+caliper(psDist,3),data=meddat,tol=.00001)
##fm2a<-fullmatch(absdist+caliper(psDist2,3),data=meddat,tol=.00001)

xb1<-xBalance(update(balfmla,.~.+pScore2),
	      strata=list(raw=NULL,fm1=~fm1,fm2=~fm2,pm2=~pm2),
	      data=meddat,
	      report=c("std.diffs","z.scores","adj.means",
		       "adj.mean.diffs", "chisquare.test","p.values"))
xb1$overall

summary(fm1,min.controls=0,max.controls=Inf)
table(matched(fm1))
summary(fm2,min.controls=0,max.controls=Inf)
table(matched(fm2))


@

\item Here are two treatment effects estimated using the harmonic mean
  weighting that I like (plus the unadjusted effect):

<<>>=

lmRaw<-lm(HomRate08~nhTrt,data=meddat)
lmFm1<-lm(HomRate08~nhTrt+fm1,data=meddat)
lmFm2<-lm(HomRate08~nhTrt+fm2,data=meddat)

ateRaw<-coef(lmRaw)[["nhTrt"]]
ateFm1<-coef(lmFm1)[["nhTrt"]]
ateFm2<-coef(lmFm2)[["nhTrt"]]

c(ateRaw,ateFm1,ateFm2)

@

If we tested a hypothesis about one of these effects, what would it mean?
(i.e. Imagine that we said, $H_0: \tau=0$ and got $p=.02$, how would you
explain this to an educated person who is not a statistician? What about
$p=.8$?) If we produced a confidence interval, and it did not contain 0 what
would it mean? If we produced a confidence intervals and it did contain 0,
what would it mean?

\item Let us start with a confidence interval. Recall the general definition
  for large samples (i.e. the Normal theory definition):
  $\hat{\tau} \pm z_{\alpha/2}\text{SE}(\hat{\tau})$ where $z_{\alpha/2}
  \approx 2$ in large samples ($z$ refers to a standard Normal/Gaussian
  deviate). What this means is that in large samples, with sufficient other
  information, we feel comfortable with the presumption that the randomization
  distribution of our estimator is Normal. So, that leaves us with the
  question about how to produce an estimate of the standard error (i.e.
  $\widehat{\text{SE}(\hat{\tau})}$). What would a standard error mean in this
  case?

\item  So, now we have some sense about why people like the ATE (1) it
  represents something about unobserved comparisons in a fixed or finite
  population of scientific interest (i.e. the sample on hand or the
  experimental pool) and (2) the sample difference of means is an unbiased
  estimator of the ATE when you have a randomized experiment. When you do not
  have a randomized experiment, but have created a matched design that is hard
  to distinguish from a randomized experiment, the ATE is also a popular
  concept and sensitivity analyses are used to address concerns about the lack
  of randomization. So, the adjusted differences of means that we reported
  earlier can be understood as estimates of the ATE.

  But, we'd like more than an unbiased estimator. We'd like to talk about how
  much information we have about this mean. After all, a mean taken on an
  experiment of size 2 ought to tell us less about the unobserved potential
  outcomes than a mean from an experiment of size 1000. We know that the mean
  difference estimator is unbiased (and other results hold for the stratified
  case). In a large sample, we presume that the distribution of means will
  closely approximate a Normal distribution by the Central Limit Theorem (so
  we can use a Normal distribution for confidence intervals or hypothesis
  tests). But which Normal distribution? What is the variance of this
  distribution?  \citep{linlr:2011}   suggests that the "iid" (independent and
  identically distributed observations)-based OLS standard error is
  incorrect and shows that the HC2 (heteroskedasticity consistent) standard
  error can be derived from the same kind of reasoning that we used for
  unbiasedness above (i.e. not considering sampling from some population, but
  considering re-running the experiment on the same pool of subjects).

  So, here is one approach. Explain what is going on.

<<results="hide">>=
## You may need to install these packages
library(sandwich)
library(lmtest)


## Here is the estimated variance of the estimated ATE using the standard iid
## assumptions
vcov(lmFm2)

## And here is the version recommended by Neyman and Lin
vcovHC(lmFm2,type="HC2")

cbind(iidSE=sqrt(diag(vcov(lmFm2))),
      NeymanSE=sqrt(diag(vcovHC(lmFm2,type="HC2")))
      )[2,]

      @

      Interpret those standard errors in substantive terms.

      \begin{comment}
      Recall what a standard error is in the context of randomization inference:
      the standard error tells us about variation across repeated randomizations in
      the same experimental pool.
      \end{comment}

      \item Now, let's use this standard error to get a confidence interval.
      Interpret these confidence intervals in substantive terms.

      No need to comment every line in this next function, although I bet you'd
      understand it all. I just needed to allow the confint function to take a
      custom variance-covariance matrix of the coefficients argument ("thevcov"). I
      encourage you to cut and paste this from the .Rnw file rather than from the
      .pdf file to make your lives easier:

<<newconfintfn>>=
source(url("http://jakebowers.org/ICPSR/confintHC.R"))
@


Or load the following (they should be the same function)
<<eval=FALSE>>=
confint.HC<-function (object, parm, level = 0.95, thevcov, ...) {
  ## a copy of the confint.lm function adding "thevcov" argument
  cf <- coef(object)
  pnames <- names(cf)
  if (missing(parm))
    parm <- pnames
  else if (is.numeric(parm))
    parm <- pnames[parm]
  a <- (1 - level)/2
  a <- c(a, 1 - a)
  fac <- qt(a, object$df.residual)
  pct <- stats:::format.perc(a, 3)
  ci <- array(NA, dim = c(length(parm), 2L), dimnames = list(parm,
							     pct))
  ## The original version extracts  the var-cov matrix itself
  ## ses <- sqrt(diag(vcov(object)))[parm]
  ses <- sqrt(diag(thevcov))[parm]
  ci[] <- cf[parm] + ses %o% fac
  ci
}
@

And here are the respective confidence intervals:

<<thecis>>=

therandci<-confint.HC(lmFm2,level=.95,parm="nhTrt",thevcov=vcovHC(lmFm2,type="HC2"))
theiidci<-confint(lmFm2,level=.95,parm="nhTrt")

@

\item But, do we trust these standard errors from the analytic results of the
  lienar model to really capture how our $\hat{\bar{\tau}}$ would vary across
  repetitions of the experiment with these same units (holding potential
  outcomes fixed)? For example, the Freedman, Pisani and Purves variance
  derivation involved finite population corrections for the idea that we are
  not sampling from an infinite population but have a fixed experimental pool
  from which we are ``sampling'' the control group (or treatment group). And
  what about the assumption that the experiment-to-experiment distribution of
  the $\hat{\bar{\tau}}$ ought to be closely approximated by a Normal/t
  distribution?

  Let's check it out. Use the info below to calculate the standard error of
  the $\hat{\bar{\tau}}$. You can just calculate the SEs and compare them to
  the SEs arising above.

<<>>=
## First using the Freedman, Pisani and Purves derivation also discussed in
## the Gerber and Green reading

y0<-meddat$HomRate08
y1<-y0-.3
Y<-with(meddat, nhTrt*y1+(1-nhTrt)*y0)
V<-var(cbind(y0,y1))
varc<-V[1,1]
vart<-V[2,2]
covtc<-V[1,2]
N<-length(y0)
n<-sum(meddat$nhTrt)
m<-N-n

varestATE<-((N-n)/(N-1))*(vart/n) + ((N-m)/(N-1))* (varc/m) + (2/(N-1)) * covtc

## And the feasible version (where we do not observe the potential outcomes)
varYc<-with(meddat,var(HomRate08[nhTrt==0]))
varYt<-with(meddat,var(HomRate08[nhTrt==1]))
fvarestATE<-(N/(N-1)) * ( (varYt/n) + (varYc/m) )
aCI<-ateFm2 + c(-1,1) * qt((1-.05/2),6) * sqrt(fvarestATE)

@

%%\begin{comment}

Notice that different versions of the SE differ --- they differ because some
ignore the finite "population" or the idea that we are re-assigning treatment
without replacement. Others do not use the variance of each group separately,
or they do not include the covariance (because it is never really observed if
you remember the Freedman, Pisani and Purves reading).

%%\end{comment}

\item Now, we already calculated the $\hat{\bar{\tau}}$ across many
  repetitions before to assess bias.  Report an approximation to the standard
  error using one of the simulations that we already did. \emph{Hint:} The
  standard error is supposed to be the typical amount by which our
  $\hat{\bar{\tau}}$ would vary from assignment to assignment.



<<>>=

simATE<-function(){
  Znew<-sample(meddat$nhTrt)
  Y<-Znew*y1+(1-Znew)*y0
  coef(lm(Y~Znew))[["Znew"]]
}

set.seed(12345)
estAteDist<-replicate(1000,simATE())

sd(estAteDist) ## Approximated SE from simulation
sqrt(varestATE) ## true SE given a large sample and known potential outcomes
sqrt(fvarestATE) ## finite sample adjusted estimate of the SE (like the sd() above)
sqrt(diag(vcov(lmFm2)))[2] ## iid SE
sqrt(diag(vcovHC(lmFm2,type="HC2")))[2] ## HC2 SE
@


\end{enumerate}

\section{Sensitivity Analysis}

So, you now know how to produce and evaluate matched designs and how to
estimate and interpret and evaluate average treatment effects and confidence
intervals. In the end, however, Metrocable was not randomly assigned.

\begin{enumerate}

  \item Now let's think about the method \citet{hosman2010} propose for
    sensitivity analysis. Their approach uses two parameters: $\teeW$
    and $\pcor$. What do these parameters mean?

    \begin{comment}

      $\teeW$ is the
      relationship between treatment assignment and the imagined covariate in
      question (it is like Rosenbaum's $\Gamma$ but on the $t$-scale)

      $\pcor$ is  the extent to which this covariate is prognostic or predictive of
      outcomes. It relates the unobserved covariate to outcomes.

    \end{comment}

  \item Why use two parameters rather than one?

    \begin{comment}
      Recall that statistical inferences about treatment effects will
      only change if the unobserved confounder is related to both
      outcome and treatment. So, two parameters more closely map onto
      how we have already learned about omitted variable bias --- and,
      it seems, there maybe times when a high $\teeW$ may not, in the
      end, change the inference if it is not paired with a high $\pcor$.

    \end{comment}

  \item How do \citet{hosman2010}  suggest that you choose
    $\teeW$ and $\pcor$ for use in simple linear models (without matching)?

    \begin{comment}
      By calibrating choice of these parameters against those actually
      seen in your dataset.
    \end{comment}

  \item How do they suggest you choose these parameters if you did
    post-stratification on a propensity score?

    \begin{comment}
      Re-fit the propensity score model leaving out each covariate in
      turn, etc...
    \end{comment}

  \item Let's try this. We would like to calculate  $\teeW$ and $\pcor$ for
    each variable used in the matching (and/or maybe some others). In the
    article, they distinguish between numeric variables (where we can just
    grab the $t$-statistic from a linear model) and multicategory/factor
    variables (for which a simple $t$-statistic from a linear model is not
    available and is approximated by an $F$-statistic).

<<defspecpar,eval=TRUE,echo=TRUE>>=
source(url("http://jakebowers.org/ICPSR/hhhsensfns.R"))
@



How would you interpret the following result for one term? \emph{Hint:} You
might want to download the code file that we sourced above and/or refer back
to the article.

<<usespecpars,cache=TRUE>>=

## first turn all logical variables into numeric variables
## because of funny problems with the formula handling
medadjdat<-meddat[,all.vars(balfmla)[-1]]

adjcovs<-names(medadjdat)

## To remind us
## we created pScore2 via brglm and then matched with
## fm2<-fullmatch(psDist2+caliper(psDist2,3),data=meddat,tol=.00001,min.controls=.25,max.controls=4)


specpars.ps(dat=meddat,covs=adjcovs,dropcov="nhOwn",
	    type=1, outcome="HomRate08", treatment="nhTrt",
	    fitfunction=brglm,thecaliper=3,
	    fmcontrol= list(tol=.00001,min.controls=.25,max.controls=4))


specpars.ps(dat=meddat,covs=adjcovs,dropcov="nhClass",
	    type=2, outcome="HomRate08", treatment="nhTrt",
	    fitfunction=brglm,thecaliper=3,
	    fmcontrol= list(tol=.00001,min.controls=.25,max.controls=4))



specps.res<-sapply(adjcovs,function(thecov){
		   message(thecov) ## just to see what it is doing
		   specpars.ps(dat=meddat,covs=adjcovs,dropcov=thecov,
			       type=is.factor(meddat[,thecov])+1,
			       outcome="HomRate08", treatment="nhTrt",
			       fitfunction=brglm,thecaliper=3,
			       fmcontrol= list(tol=.00001,min.controls=.25,max.controls=4))
	    })

rownames(specps.res) = c("r.par", "t.w", "b", "se.b", "df", "add.b", "add.se.b")

@

<<echo=FALSE>>=
tmp<-round(specps.res[,"HomRate03"],4)
@


\begin{comment}

  So, let us consider the column for the baseline outcome: The row are, in
  order from top to bottom, $\pcor$, $\teeW$, the treatment effect (i.e. coef
  on nhTrt) when this variable is \emph{not} included in the propensity
  model/matching; the finite-sample (HC2approx) standard error on this coef;
  the df for that regression (mainly relevant for factor variables); and the
  coef for nhTrt when this term is included in the matching/fixed-effects and
  corresponding standard error.

  The most important pieces are the two sensitivity parameters. We can see that
  we decrease "unexplained" variation in 2008 Homicide Rate by about
  \Sexpr{tmp["r.par"]} when we include this term versus when we do not include
  this term. And we see that this term is moderately strongly  related to the
  neighborhood received the Metrocable intervention ($t$-statistic of about
  \Sexpr{tmp["t.w"]}).

\end{comment}


\item We can see the range of both sensitivity parameters in this plot.
<<eval=FALSE>>=
plot(specps.res["r.par",],abs(specps.res["t.w",]))
## this next command allows you to click on points and get a label
## I think a double click or an escape stops the identification process
identify(specps.res["r.par",],abs(specps.res["t.w",]),
	 labels=colnames(specps.res),cex=.6)
	 @

What does this plot suggest about the potential influence of unobserved
covariates which are like the ones that we have used in this dataset?

\item Ok. Now, to continue with the \cite{hosman2010} development. They propose to report
sensitivity intervals. Interpret the output.

<<>>=
adf<-mean(specps.res["df",])
t95<-qt(.975,df=adf) ## Skipping their bootstrapping step.

sensintervals<-sapply(colnames(specps.res),function(nm){
		      c(abs(specps.res["t.w",nm]),specps.res["r.par",nm],
			make.ci(meddat,specps.res,nm,t95,specps.res["r.par",nm]))
	 })

colnames(sensintervals)<-colnames(specps.res)
rownames(sensintervals)<-c("t.w","r.par","l.bound","u.bound")

## make the table long rather than wide
theintervals<-t(round(sensintervals,4))

## The two CIs for the treatment effect itself

aCI

therandci


theintervals[order(theintervals[,"t.w"],theintervals[,"r.par"],decreasing=TRUE),]##[1:10,]

@

\begin{comment}

  So, this is slightly different from \citet{hosman2010}: I don't have
  covariates which I excluded wholesale from the analysis to play the role of
  unobserved confounders. Rather, I use the variables that we had included.
  So, we might read the following table as telling about the sensitivity of
  our treatment effect estimation to excluded variables with characteristics
  like those of the included variables.

<<plotcis,fig.width=8,fig.height=8,out.width='.6\\textwidth',fig.keep='last'>>=
par(mfrow=c(1,1),mar=c(3,6,0,0),mgp=c(1.5,.5,0),oma=c(0,2,0,0))
plot(range(theintervals[,3:4]),c(1,nrow(theintervals)),type="n",axes=FALSE,
     ylab="",
     xlab="estimated ATE 95% CI")
segments(theintervals[order(theintervals[,"t.w"]),'l.bound'],1:nrow(theintervals),
	 theintervals[order(theintervals[,"t.w"]),'u.bound'],1:nrow(theintervals))
axis(1)
segments(therandci[1,1],nrow(theintervals)/3,
	 therandci[1,2],nrow(theintervals)/3,
	 lwd=3)
text(therandci[1,1]-.2,nrow(theintervals)/3,"Est CI")
axis(2,at=1:nrow(theintervals),labels=row.names(theintervals),las=2)
mtext(side=2, "Confound Type",outer=TRUE)


@

\end{comment}


\end{enumerate}

% Next: how to know if your test is a good test (analogy to bias)



\bibliographystyle{apalike}
\bibliography{/Users/jwbowers/Documents/PROJECTS/BIB/big}




\end{document}
