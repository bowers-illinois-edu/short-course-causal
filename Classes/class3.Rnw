

% For LaTeX-Box: root = class3.tex
% search and replace keep.comment=FALSE to TRUE for commented version
\documentclass[10pt,letterpaper]{article}
%\usepackage{fontspec}
\usepackage{ulem}

\title{Session 3 --- Matching Short Course}

\author{Jake Bowers}

\usepackage{../Styles/ps531}


\includeversion{comment}
\markversion{comment}
%\excludeversion{comment}

\begin{document}
\normalem

<<include=FALSE,cache=FALSE>>=
opts_chunk$set(tidy=TRUE,echo=TRUE,results='markup',strip.white=TRUE,fig.path='figs/fig',cache=FALSE,highlight=TRUE,width.cutoff=132,size='footnotesize',out.width='1.2\\textwidth',message=FALSE,comment=NA)

options(width=110,digits=3)
@

\maketitle

\begin{enumerate}
    \setcounter{enumi}{-1}
  \item  We'll continue to work with the Cerd\'{a} et al data. We completed
    some statistical and conceptual preliminaries on Monday. Yesterday, we
    started work on making simple matched designs after we confronted some of
    the problems with the use of linear models for statistical adjustment.

<<>>=
load(url("http://jakebowers.org/Matching/meddat.rda"))
## or, if that doesn't work
## meddat<-read.csv(url("http://jakebowers.org/Matching/meddat.csv"))
@


Before we begin you should create the outcome variables and useful formulas
<<>>=
meddat$HomRate03<-with(meddat, (HomCount2003/Pop2003)*1000)
meddat$HomRate08<-with(meddat, (HomCount2008/Pop2008)*1000)
newformula<-reformulate(c("nhTrt",names(meddat)[5:24],"HomRate03"),response="HomRate08")
balfmla<-update(newformula,nhTrt~.-nhTrt)


@

\item Last time you worked on understanding what optimal full-matching
  (without replacement) was doing (the use of a distance matrix, the
  difference between greedy and optimal matching, the difference between
  matching with and without replacement). As a review, can you explain what is
  going on in this next bit of code? What are your thoughts about the
  strengths and weakness of the balance testing procedure?


<<results="markup">>=
# install.packages("optmatch",dependencies=TRUE)
library(optmatch)
tmp <- meddat$HomRate03
names(tmp) <- rownames(meddat)
absdist <- match_on(tmp, z = meddat$nhTrt)
fm0<-fullmatch(absdist,data=meddat)
summary(fm0)
@

<<assessbalance,results="markup">>=
library(RItools)
xb0<-xBalance(balfmla,
	      strata=list(raw=NULL,fm0=~fm0),
	      data=meddat,
	      report=c("std.diffs","z.scores","adj.means",
		       "adj.mean.diffs", "chisquare.test","p.values"))

xb0$overall
## xb0$results
@

\item Before we proceed, I think that we should drop the distance to city center variable. What
  do you think? Why would we drop it? Why would we keep it? Maybe this table
  helps.

<<results="markup">>=
with(meddat,table(nhDistCenter,nhTrt,exclude=c()))
@

If we drop it, the unadjusted and simple baseline-matched balance changes.

<<>>=
balfmla<-update(balfmla,.~.-nhDistCenter)
xb0a<-xBalance(balfmla,
	       strata=list(raw=NULL,fm0=~fm0),
	       data=meddat,
	       report=c("std.diffs","z.scores","adj.means",
			"adj.mean.diffs", "chisquare.test","p.values"))

xb0a$overall
## plot(xb0a)
@

\item Some might be concerned that we are not checking for balance on the
  distributions of these variables and instead are focusing too much on means.
  One way to answer those concerns is by breaking these variables into parts,
  here is one approach. What is going on here? (you might google for "ns
  splines" to see some pictures).

<<results="markup",tidy=FALSE>>=
## How many variables have more than a few values and are numeric?
sapply(meddat[,all.vars(balfmla)],function(x){ length(unique(x)) })
sapply(meddat[,all.vars(balfmla)],function(x){ class(x) })

library(splines)
toExpandNms<-all.vars(balfmla)[3:21]
reformulate(toExpandNms)
tmp<-paste("ns(",toExpandNms,",df=4)",sep="") ##,collapse="+")
balfmlaBig<-reformulate(tmp,response="nhTrt")

xb0b<-xBalance(balfmlaBig,
	       strata=list(raw=NULL,fm0=~fm0),
	       data=meddat,
	       report=c("std.diffs","z.scores","adj.means",
			"adj.mean.diffs", "chisquare.test","p.values"))

xb0b$overall
@

\item We have moderate balance across this set of variables in this study, but
  we would like better balance across more variables. The problem, of course,
  is that we cannot match on them all at once. As you saw in the Rosenbaum
  reading, the current set of solutions involves adding a step to the process:
  to produce a one-dimensional score from the many-dimensional covariate
  matrix. Common scores either focus on preserving
  essential/important information about the relationships among the covariates
  (ex. using Mahalanobis distance) or relationships between the covariates and
  the treatment (ex. using a propensity score). Today we compare the two
  approaches starting with Mahalanobis distance. Can you explain this distance
  metric to yourself or others? (Perhaps this discussion would be useful:
  \url{http://stats.stackexchange.com/questions/62092/bottom-to-top-explanation-of-the-mahalanobis-distance})

<<results="markup">>=

mahalDist<-match_on(balfmla,data=meddat)
str(mahalDist)
mahalDist[1:5,1:5]

fm1<-fullmatch(mahalDist,data=meddat)

summary(fm1)

xb2<-xBalance(balfmla,
	      strata=list(raw=NULL,fm1=~fm1),
	      data=meddat,
	      report=c("std.diffs","z.scores","adj.means",
		       "adj.mean.diffs", "chisquare.test","p.values"))

xb2$overall

@


<<eval=FALSE,out.width='.5\\textwidth',tidy=FALSE>>=
plot(xb2,
     which.vars=dimnames(xb2$results[order(xb2$results[,"p","raw"]),"p",])[["vars"]]
     )

@

\item Now, above we used a Mahalanobis Distance to (1) reduce the dimension of
  the covariate space (i.e. from \Sexpr{length(all.vars(balfmla))-1}
  covariates to 1 score) and (2) to represent the distance or relationships
  between the treated and control units.  Another common dimension reduction
  metric is the propensity score. The intuition behind the propensity is that
  we only care about covariate imbalance insofar as it relates to treatment
  assignment (and the outcome, because only when a covariate relates to both
  do we see confounding/"omitted variable bias").  Here is how one might
  calculate a propensity score, match on it, and assess balance.


<<results="markup">>=

glm1<-glm(balfmla,data=meddat,family=binomial(link="logit"))

## See fitted(glm1) for more signs that this is bad.
## or you can look at some diagnostic plots:
## par(mfrow=c(2,2))
## plot(glm1)

@

Notice that this is a terrible fitting model!!(The warnings are not the reason
it is bad. See \texttt{fitted(glm1)} or the coefficients themselves for a hint
of what is going on.) What to do?? This phenomenon is known as "separation"
often happens when the number of variables in the model is close to the number
of units in the sample and/or if some of the variable predict the treatment
assignment very well. Here, I propose to use a kind of modern version of ridge
regression (the "elastic net" penalized linear model \citep{zou2004regression}) to fit the model
(alternatively, you could try to use "bias-reduced" logistic regression in the
package \texttt{brglm} or
Gelman's \texttt{bayesglm} version of a simple Bayesian logistic model).

<<>>=
## install.packages("glmnet",dependencies=TRUE)
library(glmnet) ## you may need to install this package plus dependencies

X<-model.matrix(update(balfmla,.~.-1),data=meddat)
y<-meddat$nhTrt
ridge1cv<-cv.glmnet(X,y,family="binomial",alpha=0,type.measure="class")
@

The elastic net model finds the vector of $\beta$ that satisfied this
objective function:

\begin{equation*}
  \hat{\bbeta}(\text{Elastic-Net}) = \text{arg min}_{\bbeta} %
  \sum_{i=1}^n (y_i - \bX_i \bbeta)^2 + %
  \lambda \sum_{j=1}^p (\gamma \beta_j^2 + (1-\gamma) |\beta_j|) %\label{eq:enet}
\end{equation*}

Here is a graphical demonstration of how the coefficients in the propensity
model change depending on the penalty chosen. This is a ridge regression, also
known as an $L_2$ penalized linear model, so the coefficients move smoothly
toward zero. The model which minimizes misclassification error is shown with
the vertical line.

<<out.width='.5\\textwidth'>>=
exenet1<-ridge1cv$glmnet.fit
par(mgp=c(1.5,.3,0),oma=rep(0,4),mar=c(3,3,3,0))
plot(exenet1,xvar="lambda",
     label=FALSE,
     axes=TRUE,
     xlab=expression(paste(log(lambda)," given ",alpha==0)),cex.lab=1.5,
     ylab=expression(beta[j]))
labcoefs<-coef(exenet1)[-1,ncol(coef(exenet1))]
text(log(exenet1$lambda)[ncol(coef(exenet1))],labcoefs ,names(labcoefs),
     adj=c(0,.5),cex=.9)
mtext(side=3, "# Non-Zero Coefs.",line=1.5,cex=1.5)
abline(v=log(ridge1cv$lambda.min))
@


<<>>=
## We tend to match on the linear predictor (XB) rather than on the inverse
## link function transformed linear predictor inv.logit(XB).
## pScore<-predict(ridge1cv,newx=X,s="lambda.min",type="response")[,1]

## Compare the unpenalized model to the penalized model:
## cbind(predict(glm1),pScore)
pScore<-predict(ridge1cv,newx=X,s="lambda.min")[,1]
all.equal(names(pScore),row.names(meddat))
meddat$pScore<-pScore
@



<<eval=FALSE>>=
plot(density(meddat$pScore[meddat$nhTrt==1]),xlim=range(meddat$pScore))
lines(density(meddat$pScore[meddat$nhTrt==0]),col="blue")
@

<<results="markup">>=

## psDist<-match_on(glm1) ## I want to see balance on the p-score itself.
## meddat$pScore<-predict(glm1)
psDist<-match_on(nhTrt~pScore,data=meddat)
fm2<-fullmatch(psDist,data=meddat)
summary(fm2,min.controls=0,max.controls=Inf)

xb3<-xBalance(update(balfmla,.~.+pScore),
	      strata=list(raw=NULL,fm1=~fm1,fm2=~fm2),
	      data=meddat,
	      report=c("std.diffs","z.scores","adj.means",
		       "adj.mean.diffs", "chisquare.test","p.values"))

xb3$overall

@


\item So far we have not deleted any observations. However, sometimes it makes
  sense to reduce the sample size in order to improve our attempts to
  eliminate alternative explanations. The way that we do this involves
  stating that certain pairs of observations ought never to be included in the
  same matched set, we make these statements with the \texttt{caliper}
  function (the idea is that we set a range outside of which we declare a
  match to be bad). Here is an example. Can you explain this? (perhaps the
  help page for \texttt{caliper} will be useful?)

<<>>=
## Notice that we have some very large distances in the psDist matrix (6sds!!)
summary(as.vector(psDist))

## Also, require that neighborhoods be relatively close in terms of baseline
## homicide rates
summary(as.vector(absdist))

## Make a new mahalanobis distance matrix including a caliper based on the
## propensity score and another caliper based on the baseline outcome.
mhWithPsCaliper<-mahalDist+caliper(psDist,1)+caliper(absdist,1)
as.matrix(mhWithPsCaliper)[1:5,1:10]

@

What kind of balance do we see with this match? How many observations do we
lose? Which observations did we lose? How are they different from the others?
(i.e. What kind of neighborhoods are we limiting our inferences to?)

<<>>=

fm3<-fullmatch(mhWithPsCaliper,data=meddat)
summary(fm3,min.controls=0,max.controls=Inf)

all.equal(names(fm3),row.names(meddat))

meddat$fm3<-fm3
@


<<results="markup">>=

## These next lines print out too much. How could we limit what we look at? Or
## look at the difference between these neighorhoods more succinctly?

meddat[unmatched(meddat$fm3),]
summary(meddat[unmatched(meddat$fm3),all.vars(balfmla)])
summary(meddat[matched(meddat$fm3),all.vars(balfmla)])

@


\item Now that we have some sense about how to interpret balance assessments
  and make matched designs, can we make a better matched design? What is the
  best you can create?


\item Once we have matched design that we can defend, we want to estimate a
  causal effect and/or test a hypothesis about a causal effect. Let us focus
  for the moment on estimation. Here are three proposals that arise from ideas
  I've read in \cite{freedman2008rae}, \citep{freedman2008rdn}, and
  \citep{linlr:2011}, and \citet{miratrix2012adjusting}.

<<>>=

lmAte1<-lm(HomRate08~nhTrt+fm3,data=meddat)
lmAte2<-lm(HomRate08~nhTrt*fm3,data=meddat)
lmAte3<-lm(HomRate08~nhTrt,data=meddat,subset=matched(fm3))

Ate1<-coef(lmAte1)[["nhTrt"]]
Ate3<-coef(lmAte3)[["nhTrt"]]

makeAte2<-function(thelm){
  preddat<-na.omit(data.frame(nhTrt=rep(c(0,1),nrow(meddat)),fm3=rep(meddat$fm3,each=2)))
  haty1<-predict(thelm,newdata=subset(preddat,subset=nhTrt==1))
  haty0<-predict(thelm,newdata=subset(preddat,subset=nhTrt==0))
  mean(haty1)-mean(haty0)
}

Ate2<-makeAte2(lmAte2)
## Another version:
options(show.signif.stars=FALSE)

xbAte4<-xBalance(nhTrt~HomRate08,
		 strata=list(fm3=~fm3),
		 data=meddat[!is.na(meddat$fm3),],
		 report=c("std.diffs","z.scores","adj.means",
			  "adj.mean.diffs", "chisquare.test","p.values"))

Ate4<-xbAte4$results[,"adj.diff",]

@

So,  which do we prefer? I setup this simulation to assess them. What is going
on here?
<<cache=TRUE>>=
## define a function that re-assigns treatment with equal probability within
## matched set or experimental block
set.assignment<-function(set,z){
  ## library(mosaic);  shuffle(z,within=pair) is more elegant but harder to understand as code
  unsplit(lapply(split(z,set),sample),set)
}


## define a function which reveals a difference in observed outcome and calculates
## a different mean difference given a different treatment vector
newExperimentAndATE<-function(thez,y1,y0,thedata){
  message(".",appendLF=FALSE) ## To see how fast this is running

  newobsy<-thez*y1+(1-thez)*y0
  lmAte1<-lm(newobsy~thez+fm3,data=thedata)
  lmAte2<-lm(newobsy~thez*fm3,data=thedata)
  lmAte3<-lm(newobsy~thez,data=thedata,subset=matched(fm3))

  Ate1<-coef(lmAte1)[["thez"]]
  Ate3<-coef(lmAte3)[["thez"]]

  makeAte2<-function(thelm){
    preddat<-na.omit(data.frame(thez=rep(c(0,1),nrow(thedata)),fm3=rep(thedata$fm3,each=2)))
    haty1<-predict(thelm,newdata=subset(preddat,subset=thez==1))
    haty0<-predict(thelm,newdata=subset(preddat,subset=thez==0))
    mean(haty1)-mean(haty0)
  }

  Ate2<-makeAte2(lmAte2)

  ## Another version:
  options(show.signif.stars=FALSE)

  xbAte4<-xBalance(thez~newobsy,
		   strata=list(fm3=~fm3),
		   data=thedata[!is.na(thedata$fm3),],
		   report=c("std.diffs","z.scores","adj.means",
			    "adj.mean.diffs", "chisquare.test","p.values"))
  Ate4<-xbAte4$results[,"adj.diff",]
  return(c(Ate1=Ate1,Ate2=Ate2,Ate3=Ate3,Ate4=Ate4))
}



y0<-meddat$HomRate08[matched(meddat$fm3)]
y1<-y0-.3 ## for now presume the same effect for all neighborhoods

newExperimentAndATE(thez=with(meddat[matched(meddat$fm3),],set.assignment(fm3,nhTrt)),
		    y1=y1,y0=y0,
		    thedata=meddat[matched(meddat$fm3),])

set.seed(20140715)

library(parallel)
##ateDists<-replicate(1000,
ateDists<-mclapply(1:10000,function(i){
		   newExperimentAndATE(thez=with(meddat[matched(meddat$fm3),],
						 set.assignment(fm3,nhTrt)),
				       y1=y1,y0=y0,
				       thedata=meddat[matched(meddat$fm3),])},
		   mc.cores=detectCores()
		   )


ateDistsArr<-simplify2array(ateDists)

apply(ateDistsArr,1,summary)
apply(ateDistsArr,1,sd)



@

\item Notice that I set the true relationship to be constant across matched
  sets. I wonder what would happen if some sets had one relationship and other
  sets had another? Here is some initial evidence about the way that the effect
  might vary across sets.


<<results="markup">>=

makeAteBySet<-function(thelm){
  preddat<-expand.grid(nhTrt=c(0,1),fm3=levels(fm3))
  haty1<-predict(thelm,newdata=subset(preddat,subset=nhTrt==1))
  haty0<-predict(thelm,newdata=subset(preddat,subset=nhTrt==0))
  return(haty1-haty0)
}

ateBySet<-makeAteBySet(lmAte2)

ateBySet2<-sapply(split(meddat,meddat$fm3),function(dat){
		  with(dat,mean(HomRate08[nhTrt==1])-mean(HomRate08[nhTrt==0]))
		   })

ateBySet

ateBySet2

@



\end{enumerate}


% Next: how to know if your test is a good test (analogy to bias)



\bibliographystyle{apalike}
\bibliography{/Users/jwbowers/Documents/PROJECTS/BIB/big}




\end{document}
