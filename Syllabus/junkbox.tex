
This course focuses on learning about causal claims from observational
studies. This means that we should first become clear about `cause' (as
opposed to association) and also about `observational studies' (as opposed to
experimental studies). Therefore we begin the class by discussing the
potential outcome notation for causal inference and how the \textbf{design} of
randomized experiments allow us to learn about causal relations with some
special confidence. We introduce the idea of an unbiased estimator of a causal
effect and convince ourselves that a randomized experiment allows for unbiased
estimation of an average treatment effect. 


How does the linear model ``control for'' or ``adjust for'' potentially
confounding variables when assessing a relationship between some outcome and
some explanatory or causal variable? How can we know whether we have
``controlled for'' enough? How worried should we be about the influence of
functional form on our causal inferences? How does a randomized experiment do the
same task using design? 




\emph{Statistical} inference for \emph{causal} quantities (or for
\emph{causal} inferences) is an answer to the question: ``If the intervention
had been re-applied, how might the results have been different?'' Or ``How
much information do I have about whether the intervention had this effect
versus that effect?''

What does it mean to do statistical inference for a matched design? To what
unobserved quantities might we want to infer? What might justify the idea that
what we observed might have been different? Here we focus on Neyman's ideas,
but we will also talk briefly about Fisher's sharp null hypothesis of no
effects.

